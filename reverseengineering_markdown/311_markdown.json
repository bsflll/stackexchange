{
  "title": "Why are machine code decompilers less capable than for example those for the CLR and JVM?",
  "link": "https://reverseengineering.stackexchange.com/questions/311/why-are-machine-code-decompilers-less-capable-than-for-example-those-for-the-clr",
  "content": "Java and .NET decompilers can (usually) produce an almost perfect source code, often very close to the original.\n\nWhy can't the same be done for the native code? I tried a few but they either don't work or produce a mess of gotos and casts with pointers.\n\n",
  "votes": "33",
  "answers": 4,
  "views": "12k",
  "tags": [
    "decompilation",
    "x86-64",
    "x86",
    "arm"
  ],
  "user": "Rolf Rolles",
  "time": "Mar 27, 2013 at 10:56",
  "comments": [
    {
      "user": "asheeshr",
      "text": "Its great that you wrote this post, however it still needs to be in the form of a Q&A. If you could turn this into a set of questions, then it would be even better :)",
      "time": null
    },
    {
      "user": "Rolf Rolles",
      "text": "Is that better?",
      "time": null
    },
    {
      "user": "Peter Andersson",
      "text": "Do you really expand on how to make recovery of high level code difficult? I would skip that part of the question and simply make this about decompilation. Your answer is very good though imo.",
      "time": null
    },
    {
      "user": "Peter Andersson",
      "text": "@IgorSkochinsky Did you just call your Hex-Rays decompiler crappy with that edit? :P",
      "time": null
    },
    {
      "user": "Igor Skochinsky",
      "text": "Well I was going with the general sentiment you can read in many of such questions :)",
      "time": null
    }
  ],
  "answers_data": [
    {
      "content": "TL;DR:  machine code decompilers are very useful, but do not expect the same miracles that they provide for managed languages.  To name several limitations:  the result generally can't be recompiled, lacks names, types, and other crucial information from the original source code, is likely to be much more difficult to read than the original source code minus comments, and might leave weird processor-specific artifacts in the decompilation listing.\n\nThese are merely a few of the accessible examples of challenges that plague machine code decompilers.  We can expect that limitations will remain for the foreseeable future.  Therefore, do not seek a magic bullet that is as effective as managed language decompilers.\n\n",
      "votes": "47",
      "user": "Smi",
      "time": "Oct 19, 2013 at 17:55",
      "is_accepted": true,
      "comments": [
        {
          "user": "Jongware",
          "text": "On 6. When code has gone through pipeline optimization, a logical sequence of single operations may get mixed with the previous and/or next logical block of operations.",
          "time": null
        }
      ]
    },
    {
      "content": "Decompilation is difficult because decompilers must recover source-code abstractions that are missing from the binary/bytecode target.\n\nThere are several types of abstractions:\n\nDecompiling native code is difficult because none of these abstractions are represented explicitly in the native code. Thus, to produce nice decompiled code (i.e., not using gotos everywhere), decompilers must reinfer these abstractions based on the behavior of the native code.  This is a difficult process, and many papers have been written on how to infer those abstractions. See Balakrishnan and Lee for starters.\n\nIn contrast, bytecode is easier to decompile because it usually contains enough information to permit type checking.  As a result, bytecode typically contains explicit abstractions for functions (or methods), variables, and the type of each variable.  The primary abstraction missing in bytecode is high-level control flow.\n\n",
      "votes": "11",
      "user": "Ed McMan",
      "time": "Mar 27, 2013 at 17:48",
      "is_accepted": false,
      "comments": []
    },
    {
      "content": "As someone who has worked on a suite of Python decompilers, I have thought about this a bit.\n\nI am going to take a different tack than other bottom-up answers; I start to describe the situation from a slightly more general and philosophical side which I think is easy to see.\n\ntl;dr:\n\nThink of this as a common situation where someone is trying to reconstruct an object from its fragmented pieces. For example why might a smashed sandcastle be harder to reconstruct than a broken ceramic vase? From this point of view, there would be no surprise. For sure, a real explanation of the sand versus ceramic problem would involve adhesion strength of the materials, properties of fracturing for the materials involved, and mending techniques. Similarly, the properties of computer languages, compilers, runtime systems, machine code and decompilation techniques can vary just as much as sand castles and ceramic vases.\n\nIn general, compilation performs a kind of information entropy loss. This may seem surprising since the source code and the binary object are supposed to be semantically identical. Yes, that is true, but only with respect to how computers understand the world.\n\nAnd although humans were the inventors or creators of computers, it seems strange that the world in which the computer operates, its \"experience\" of the world, is already so different than the experience of a human programmer.\n\nIt is easier to build a machine that just understands how to perform logic, arithmetic, read memory locations, and jump to different program states than it is to build a computer that inherently understands concepts in programming and its patterns and paradigms, algorithms, or data structures which is the level that programmers may operate in.\n\nSo in a sense, one thing that fascinates me here is the inherent problem of communicating between different forms of \"intelligence\". If it is this hard here, the simplistic idea that we would ever be able to communicate with some sort of alien life where our level of shared experience may be much less is, in my opinion almost hopeless. In addition to the problems seen in decompilation, you have additional problems with transmission speed and delay, and just topics that we might both be interested in.\n\nAs an example, using the cited \"Universal Language\" of Mathematics, if intelligent life is in a highly curved geometry where all \"parallel\" lines always touch or always diverge, then all this Euclidean Geometry would be super boring for that alien since it would have no practical use. And Shakespeare's Poetry - forget about it!\n\nAlthough the question posed here is pretty specific, it is also a bit general or maybe vague. Here are factors that go into the ease (and thus quality) that a decompiler can provide:\n\nBefore delving into each of the categories above, let me first give an analogy using that general principle that basically we are going against information entropy or information loss.\n\nI suppose this question is a bit general as was question posed. The answer partially depends on how much effort was used to create the initial object, sand castle versus a ceramic vase. One can imagine a very simple primitive sand castle that does not have a lot of detail. Maybe it is just a box of some form. That would be easy to reconstruct while an intricate sand castle would be very hard to reconstruct.\n\nThe complexity of the program is analogous to the shape of the object we are trying to reconstruct.  If your source code or machine code is small and simple, you'd might just dispense with the decompiler and try to understand the machine code.\n\nAnd the material for the end result and processes used to create it are important too.\n\nReconstructing things made out of sand is much harder than ceramic that has a couple of well-defined factures. This is like the kind of object that gets run: binary code or bytecode. For bytecode, some are higher level then others. Python bytecode, Pascal P-code or GNU Emacs bytecode is more like ceramic than ARM assembly which is more like sand.\n\nAlso, how our sand castle or vase were destroyed is important too. Was the sand castle just kicked once or was at worn away by the tide repeatedly? And was the vase just accidentally dropped on the floor once or was it smashed to little pieces by a hammer?\n\nThe analogous process here has the compilation process. Was this a \"one-pass\" compiler like CPython or a multi-stage compilation pipeline that say GNU gcc or clang do where levels of \"optimization\" have been turned on?\n\nAnd finally we get to the amount of effort or care you want to put in to recreating the initial object. If this vase is not of much value to you, you probably won't bother to do a careful and accurate job if you bother at all. Getting something approximately vase like, might be good enough for your needs.  However if the vase is precious, well then you probably will spend a lot of effort to reconstruct the original. In fact when my mother broke a vase that she considered a precious heirloom, she hired professionals from the Smithsonian Museum in Washington DC to reconstruct the part of the vase that was broken.\n\nHaving the above out of the way, we can get to the nitty gritty. Just as with sand castles versus ceramic vases, I'll focus on two examples which are at opposite extremes.\n\nBut first some factors to consider.\n\nDecompilation quality depends on:\n\nI will give an example that I know very well.\n\nMany people have noted that this decompiles very well. Here are the factors why this is so:\n\nPython stores docstring comments in the code which is nice for humans looking at decompiled code, although it doesn't really change the decompilation process.\nAlso there are standards for Python code formatting. So if the human and decompiler follow the same standard and there is basically only one, the the result will look similar.\n\nThe CPython compiler is a \"one pass\" compiler that doesn't do much in the way of code improving or code transformations. But as we move to later versions, More recent versions of CPython has been doing more here.\n\nPython bytecode is also extremely high level. Variable, class, function and module names are all preserved. Python and Python bytecode are loose with type declarations.\n\nThe Python decompiler that I have been working on and maintaining makes use very specific idioms that can be found in instruction patterns. Since early Python translated everything one way (even though in theory there are many ways that it could choose), this kind of pattern driven approach is able to disambiguate between semantically identical code. For example if x: if y: ... versus if x and y. Earlier CPython create different kinds of code fragments for nested ifs versus and.\n\nHere I compare that with something at the other extreme and that I know less well. Some aspecs I know only vaguely.\n\nPython docstrings are, I suppose, a form of comment. In C any sort of document commenting is form of a comment; and all comments in C are stripped.\n\nGCC is at least 3 or 4 distinct phases. C preprocessor, C compiler to assembly, assembly, and then a linking phase. The C compiler phase though can make several passes over the AST it produces, and or the instructions it produces. There is a lot of opportunity for code mangling that may need to be undone.\n\nUnless you have symbol table generation turned on, -g, the mapping of memory locations and/or registers to names is gone. At the assembly level, any structure or type information is gone.\n\nC indention can come in one of several varieties. One could run the result through a formatter, assuming the Ghidra produces valid C. But I suspect it doesn't.\n\nIt is very possible that nested if and && could compile to the same instructions. And there may be more than one template that a compiler might use for a single construct.\n\nBut even if a particular compiler for a particular language is passed a particular set of options that compiles if and && differently, I doubt that the way Ghidra's decompiler hones in on a specific compiler's code idioms for most things.\nMy understanding is that the decompiler(s) in Ghidra are general purpose which means that they do not take into consideration that specific language, compiler or compiler options used. They work the same way on machine code whether the compiler used was gcc, clang or some vendor's C compiler like the ones IBM or Intel have. Or whether the source language was C++ instead of C.\n\nAnd this is an interesting aspect too. It takes a bit of effort to teach a decompiler about some specific compiler's quirks or habits. These things can change over time depending on the language and within a language like C, C compiler and compiler version, and the assembly language that is produced. Since there are:\n\nit probably isn't worth the maintenance effort to hone in on idiosyncrasies of a particular compiler system at a particular point in time.\n\nIn the CPython/Python case, there is basically only one compiler CPython. Bytecode does vary from major release. For example between 3.5 to 3.6 there were a number of bytecode changes. However a Major release is on the order of a year. And many times the code generation doesn't change. However Python bytecode does vary a lot more than many other kinds of bytecode. Most bytecode stays the same. JVM stays largely the same. Emacs bytecode and P-Code stay the same over longer periods of time.\n\nBecause Ghidra is general purpose, it has a general purpose algorithm for detecting control flow. In uncompyle6 and decompyle3 we can do pretty well using the patterns approach. There has been some problem with control flow in the past, but recent not public versions of this code do very well by adding basic block and dominator information as pseudo instructions of the bytecode. Basically you can think of the instructions having additional parenthesis and comma marks to help detect nesting versus sequencing around jumps.\n\nAs for maintenance, I imagine there have been more than one person working on the decompilation aspect. And I imagine that person is funded at least partially by grants if this is not a part of the person's day job.\n\nI mention the maintenance aspect because the person-hours that are spent on decompilers is much less than the person-hours spent in the compiler code generation.\n\n",
      "votes": "2",
      "user": "rocky",
      "time": "Jul 18, 2023 at 10:54",
      "is_accepted": false,
      "comments": []
    },
    {
      "content": "Java and .NET both include a feature called \"reflection\", which makes it possible to access inner workings of a class and instances thereof, using runtime-generated strings that hold the names of those features.  In C code, if one includes a structure definition:\n\n```\nstruct foo { int x,y,z,supercalifragilisticexpialidocious; };\n\n```\nexpressions that access field supercalifragilisticexpialidocious will generate machine code that accesses an int-sized object at offset 3*sizeof(int) in the structure, but nothing in the machine code will care about the name.\n\nIn C# by contrast, if code that was running with suitable Reflection permissions were to generate the string x, y, z, supercalifragilisticexpialidocious, or woof somehow, and used Reflection functions to retrieve the value of the field having that name (if any) from an instance of that type, the Runtime would need to have access to the names of those fields.  Even if a program would in fact never use Reflection to access the fields by name, there would generally be no way the Compiler could know that.  Thus, compilers need to include within executable programs the names of almost all identifiers used the source code, without regard for whether anything would ever care about whether or not they did so.\n\n",
      "votes": "1",
      "user": "supercat",
      "time": "Jun 8, 2023 at 17:22",
      "is_accepted": false,
      "comments": []
    }
  ]
}