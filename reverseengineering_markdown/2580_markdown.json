{
  "title": "Why is disassembly not an exact science?",
  "link": "https://reverseengineering.stackexchange.com/questions/2580/why-is-disassembly-not-an-exact-science",
  "content": "Newbie here.\n\nFrom Wikipedia\n\n",
  "votes": "32",
  "answers": 5,
  "views": "5k",
  "tags": [
    "disassembly",
    "reassembly"
  ],
  "user": "Einar",
  "time": "Aug 4, 2013 at 13:43",
  "comments": [
    {
      "user": "marshal craft",
      "text": "A more abstract reason could be compilation isn't 1 to 1. Every high level statement doesn't compile to a unique set of opcodes. Like in math you have a domain and range. Compilation maps the domain to the range. If the domain is a lot larger than the range then compilation isn't 1 to 1. So there isn't a general (general meaning the solution doesn't come from necessity but you make a choice) inverse or recompilation/disassembly function.",
      "time": null
    }
  ],
  "answers_data": [
    {
      "content": "To answer the first question. The biggest problem is that you can't really separate data from code. There are basically two approaches to dissasembly:\n\nDissasemblers using linear sweep start at some address and dissasemble instructions one by one until the end, without following jumps or reasoning about the dissasembled code in any way. For example SoftICE and Windbg use linear sweep. This can be a problem, because you don't know when to stop. You don't know where instructions end and where data starts. You have to rely on executable format metadata, like sections sizes, for that. That's what makes it problematic.\n\nOn the other hand, recursive traversal algorithms take jumps and calls into consideration, and the dissasembler follows jumps, only dissasembling the code that will actually get executed. For example IDA and OllyDBG use this approach. It has the benefit that it essentially knows what's code and what's data. But it has it's drawbacks, obviously. First, by pure recursive traversal, not all code will be dissasembled. For example, functions that aren't directly referenced, called in runtime by calculating their address will not be spotted. Again, engines have some heuristics to bypass this problem. \n\nTake, for example, a program that jumps back a few instructions, but instead of landing on the beginning of a previously dissasembled and executed instruction, it jumps in the middle of it. How should the dissasembler decide which one to show? Both might be valid, if that was the intention of the coder. The recursive traversal dissasembler will probably show the first dissasembled version, and just mark a jump to the middle of a function. But without you inspecting the bytes in details, it's hard to tell what will actually get executed after the jump. \n\nThere are many other examples.\n\nAnother big problem with both of these approaches is self modifying code. It just can't be done statically. \n\nThe CPU its self doesn't have any of these problem as it's executing code, in other words, it's dynamic.\n\nTo answer the second question I don't think this is a problem with CISC architectures only, some of these can be applied to RISCs too.\n\n",
      "votes": "31",
      "user": "0xea",
      "time": "Aug 9, 2013 at 16:48",
      "is_accepted": false,
      "comments": [
        {
          "user": "Ruslan",
          "text": "In fact they should say von Neumann architecture (which allows self-modification), not CISC.",
          "time": null
        },
        {
          "user": "BlueRaja - Danny Pflughoeft",
          "text": "@Ruslan: It has nothing to do with self-modification.  The point is that variable-width instructions allow for ambiguous interpretations.  Eg. you might JMP into the middle of an instruction, causing a separate, valid sequence of instructions to be read.  This is not possible on fixed-width instruction sets, which RISC (almost?) always are.",
          "time": null
        },
        {
          "user": "BlueRaja - Danny Pflughoeft",
          "text": "\"self-modifying code is not the only problem\" - Well, it's not even a problem.  On a fixed-width architecture, there should be no issue disassembling self-modifying code.  Of course, the disassembler can''t know what the assembly will look like after the runtime modifications (which is an undecidable problem), but that's outside the scope of disassembling the binary.",
          "time": null
        },
        {
          "user": "viv",
          "text": "OllyDBG uses linear sweep?",
          "time": null
        },
        {
          "user": "0xea",
          "text": "I was pretty sure it does, but I can't find the reference for that claim now. I might have had it mixed with SoftICE. Corrected in the answer. Thanks!",
          "time": null
        }
      ]
    },
    {
      "content": "In addition to the issues described in the previous answers, here's another way in which a program can have more than one disassembly.  Consider code that has the following logic (my apologies for the syntax-butchery):\n\nIn this case, whether buf is code (and therefore should be disassembled) or data (and therefore should not be disassembled) depends on the input.  Thus, the program has multiple possible disassemblies.  Note that this is because code is fundamentally indistinguishable from data and has nothing to do with RISC vs. CISC.\n\nThere's a delightful discussion of how code can be disguised as plausible-looking data in this paper:\n\n",
      "votes": "16",
      "user": "debray",
      "time": "Aug 4, 2013 at 15:26",
      "is_accepted": false,
      "comments": []
    },
    {
      "content": "I think what the comment is meant to describe is the fact that on CISC, taking the x86 as an example here, there are several possible reasonable representations of the disassembly. But I'd think this can also in part be said of typical RISC implementations, where - however - the assembler can offer something like a CISC-like mnemonic represented by a different combination of basic (RISC) opcodes.\n\nFor example the rep prefix is meaningless on many opcodes and yet has been employed in the past to make it harder for reverse engineers. However, which is the better representation for a human who is ultimately looking at the disassembly? The one with the rep prefix or the one without (which reflects more what the CPU does).\n\nIf the disassembler stays true to what it finds, it should include the rep prefix. On the other hand it's the job of the disassembler to make machine code readable for a human. So it makes perfect sense to go that extra mile and make sure that redundant prefixes are removed for brevity. Similarly multi-byte opcodes representing a nop (no operation) could be stripped completely or represented in a way to stand out from the rest (e.g. as IDA does with alignment bytes) or each as nop respectively (for the 1 and the 5 byte version alike).\n\nSo from my point of view disassembling machine code is an exact science, since for every opcode there is an exact mnemonic (let's disregard the jne/jnz duality here). Otherwise, as you point out, how would the CPU go about and figure out what to do? However, in order to give a representation to the human reverse engineer, it is sometimes (dare I say often?) post-processed for readability and comprehension. That's where disassemblers and disassemblies vary.\n\nAnother problem is that in order to follow all possible code paths heuristics have to be used (and it's one of the distinguishing features of a good disassembler). Otherwise it's hard to distinguish data from code. But to my knowledge \"CISC\" cannot be singled out for that trait, so I'd hold that this isn't the main reason for the comment.\n\n",
      "votes": "11",
      "user": "0xC0000022L",
      "time": "Aug 4, 2013 at 14:02",
      "is_accepted": false,
      "comments": []
    },
    {
      "content": "A critical issue in disassembly lies in precisely separating code from data in binary executables. To this end, you will need a static analysis that can precisely determine the targets of indirect jumps (jumps to addresses calculated at run-time). There are several examples of indirect jumps including  switch target calculation and vtables in C++. \n\nThat type of static analysis, if existed, would be able of solving the Halting Problem which is known to be undecidable. Therefore such static analysis can't exist. Based on that, disassembly tools need to rely on heuristics and/or approximations for determining indirect jumps targets. More details on that can be found in answers to a similar question here Soundness of ARM disassembly. The task of code/data separation gets even harder if binaries where obfuscated and/or have self-modifying code.\n\nHaving extracted approximations of code and data from binaries. One needs then to deal with the problem of attaching semantics to them which is also difficult. For example a struct { int i; int j;}  would look similar to an int arr [2] when accessed by instructions. Therefore, It's difficult to attach unique semantics to executed code without relying on external information such as debug symbols. \n\n",
      "votes": "5",
      "user": "Community",
      "time": "Apr 13, 2017 at 12:49",
      "is_accepted": false,
      "comments": [
        {
          "user": "Jongware",
          "text": "A larger array can be distinguished from a structure because of how its elements get accessed, and perhaps even one as small as your arr[2].",
          "time": null
        },
        {
          "user": "Codoka",
          "text": "@Jongware agree. It's just a simple example to illustrate the idea of having multiple valid meaning in disassembly.",
          "time": null
        }
      ]
    },
    {
      "content": "First of all, we need to agree on what does a correct disassembly of a binary program is. I would propose the following definition:\n\nAnother way to state it would be to say that we expose the instructions of all the possible executions of the program on every input it can take.\n\nHere we already can make a parallel with the halting problem on a Turing machine which can be defined as follow (Wikipedia):\n\nThis (apparently) very simple problem has been shown undecidable by Turing, meaning that, even if we can handle a certain amount of cases automatically with a program, some pathological cases will always escape from our program which will fail to tell if yes or no the machine/program will halt on the given input.\n\nAnd, of course, such pathological cases are infinitely many (so, there is no hope to enumerate them one after one as special cases).\n\nExploring all the possible paths of a program is indeed undecidable because of the halting problem!\n\nIndeed, a dual way to formulate the halting problem is the accessibility problem where you want to know if there is an input that allow you to reach a specific point of the program. And, knowing if a specific place in memory can be reached by the program and interpreted as an instruction (i.e. the instruction pointer takes the value of this address at some point) is an accessibility problem.\n\nSo, disassembly is undecidable.\n\nI know, I know, this is only Math... Not reality... Most of the obfuscations (voluntary or not) can be worked out and automatically removed from binary code...\n\nWell, this was essentially because people who did these obfuscations were not used to undecidable problems...\n\nImagine that you insert in your program the computation of an undecidable problem, or even, just something hard and complex enough that will break any automated reasoning applied to it.\n\nTo give an example, lets take the Collatz sequence (Wikipedia), this sequence is conjectured to always end at 1 after some time. But, the arithmetic problem behind it is so tremendously complex that this conjecture holds since about one century... This is a perfect opaque predicate to use! Of course, it might be that the proof of such conjecture exists, but this problem is complex enough to start building on it and confuse computer exploring the state space of a program.\n\nIn fact, this is the current direction of research in strong obfuscation nowadays... We are almost done with the small tricks that were used before and people start to build things on better grounded problem. Even if we still miss an equivalent of a Shannon (father of the information theory) in matter of software obfuscation to be compared with cryptology.\n\nSo, we saw that the disassembly problem is strongly linked to the halting problem. And, also, that using highly complex problems might be the next step in modern software obfuscation.\n\nI would just have a final word about the fact that current disassembly tools are probably way behind what we could do if we had to stick with the state of the Art in disassembly techniques. I am always crying in pain when I see how prehistoric are the current tools... but putting all the modern techniques into practice would require so much effort of development and maintenance that nobody seems to be ready to do it (but this is only my humble opinion).\n\n",
      "votes": "3",
      "user": "perror",
      "time": "Jun 21, 2017 at 7:45",
      "is_accepted": false,
      "comments": []
    }
  ]
}