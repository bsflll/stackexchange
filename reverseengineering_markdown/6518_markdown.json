{
  "title": "Why there are not any disassemblers that can generate re-assemblable asm code?",
  "link": "https://reverseengineering.stackexchange.com/questions/6518/basic-concept-misunderstood",
  "content": "I am struggling on this problem for around three months:\n\nHow to use disassemblers (IDA Pro and others...) to generate re-assemblable asm code and assemble it back\n\nMy experience is that:\n\nSo my questions are:\n\n",
  "votes": "32",
  "answers": 5,
  "views": "12k",
  "tags": [
    "ida",
    "disassembly",
    "x86",
    "disassemblers",
    "reassembly"
  ],
  "user": "vackermann",
  "time": "Apr 28, 2017 at 12:56",
  "comments": [
    {
      "user": "Stolas",
      "text": "The close votes are because they think this is 'opinion based'. I think this is not the case.",
      "time": null
    },
    {
      "user": "nrz",
      "text": "Have you checked VCOM Sourcer? It should produce re-assemblable disassembly for DOS and Windows. Maybe you can find it somewhere. archive.is/UJrsa",
      "time": null
    },
    {
      "user": "Trass3r",
      "text": "There are research efforts like github.com/GrammaTech/ddisasm.",
      "time": null
    }
  ],
  "answers_data": [
    {
      "content": "To elaborate:\n\nYou'll also need to extract things that are not code. Think of import tables, export tables, strings and other data. \n\nWhen you write code, this is only one part of the program. The other part is the Compiler Optimizations and data section. This makes it almost impossible to create re-compilable assembly. If you want to edit a program on assembly level I'd recommend to use windbg and LordPE.\n\n",
      "votes": "23",
      "user": "Stolas",
      "time": "Sep 12, 2016 at 7:07",
      "is_accepted": true,
      "comments": []
    },
    {
      "content": "This is from the IDA Pro book, but even IDA, as good as it is, is still in the end making guesses.  The answers here are from \"The IDA Pro Book\" by Chris Eagle.\n\nBasically, at this point it still requires human judgment.  The best analogy I've heard is that compiling a binary from source is like computing a Hash.  \n\nThis sounds to me like an interesting theoretical research question:  Can compilation truly be viewed as generating a hash signature?  My gut says \"Yes.\"  The math would be terribly intricate, and would probably have to be done with a provable language.  We typically use hashes because they aren't easy to reverse engineer.  However you can still attack hashes using things like rainbow tables, so there's one mega-project to consider.  My instinct tells me that rainbow tables on all possible binaries is NP-Complete.  \n\nAlso consider that determining data types kinda requires human judgment, and we still aren't terribly good at automating THAT kind of intelligence.  Is it possible?  Maybe.  There's a reason why smart people still make tools like IDA.  \n\nI'm new to disassembly, so I'll leave that to the big boys, but hopefully at least I answered the question on why its so difficult to do what you ask.  \n\nEagle, Chris (2011-06-16). The IDA Pro Book: The Unofficial Guide to the World's Most Popular Disassembler (Kindle Locations 151-152). No Starch Press. Kindle Edition. \n\n",
      "votes": "20",
      "user": "avgvstvs",
      "time": "Mar 8, 2014 at 16:56",
      "is_accepted": false,
      "comments": []
    },
    {
      "content": "Your question is very interesting, though, not really new. \n\nMany people already use what we call binary rewriting for profiling purposes. For example DynInst & MAQAO do that to profile applications in order to locate bottlenecks in basic blocks. Now the question you'll probably be asking yourself is how is it done ? Simple. Most available disassemblers like objdump, objconv, IDA, etc. work in standalone mode and usually print an instruction on disassembly, but others like udis86 & distorm offer an API to access the disassembled code in addition to being available in standalone mode.\nBut, what DynInst, MAQAO, and most binary rewriting tools do is disassemble a binary file and insert probes wherever it is appropriate in the data structure before reassembling the binary. Thus all necessary changes related to addresses, branches, context saving, and so on are handled properly before reassembly. \n\nWhat you must know is that it is extremely hard to write such tools. The first challenge is to write a reliable disassembler. This of course implies choosing a disassembly algorithm (linear sweep vs. recursive traversal), separating the instructions from the data (they can be mixed - shellcodes for example), and so on. Then comes the second challenge, patching the disassembled code. This is extremely tricky and I'll point out this document which should be of great help : http://www.maqao.org/publications/techreports/madras_techreport.pdf. \nIt has been written by the author of the disassembler used in MAQAO (MADRAS - Multi Architecture Disassembler Rewriter and Assembler). The interesting part about this documents is the references (over 50 and extremely helpful) and the appendices which describe the algorithms used.\n\nThough I'm not really acquainted with neither MAQAO nor DynInst I would recommend you checking publications around them (documentation, scientific papers, ...). I would also recommend you checking PEBIL (PMaCs Efficient Binary Instrumentation Toolkit), Intel's PIN, Valgrind, PLTO, Elfsh/ERESI, and Etch. \n\nMost of these tools perform binary rewriting & patching extensively and I believe are good examples of how binary rewriting can be approached. \n\nI hope my answer will help you find what you were looking for. \n\n",
      "votes": "10",
      "user": "yaspr",
      "time": "Oct 14, 2018 at 20:02",
      "is_accepted": false,
      "comments": [
        {
          "user": "lllllllllllll",
          "text": "Hello yaspr, thank you a lot for your excellent answer! IMHO, these tools you talked are basically binary re-write tools, which require a \"disassemble+patching to the original binary\" processes. However, what I am asking is that a tool can support \"disassemble+re-assemble on the disassembled asm code\".",
          "time": null
        },
        {
          "user": "lllllllllllll",
          "text": "IMHO, things would become much easier when you just patch on original binaries, in which case you don't need to (heuristically) handle tedious concrete memory addresses generated in the disassembled asm code.",
          "time": null
        },
        {
          "user": "yaspr",
          "text": "Well, MADRAS is what you need then. It was designed not only to patch to disassemble & reassemble the disassembled code with or without patching !",
          "time": null
        },
        {
          "user": "lllllllllllll",
          "text": "Hello yaspr, I have read the techreport of MADRAS you attached, and I noticed these things: 1. MADRAS can only work on x86-64 2. It require (optional)debug info!!",
          "time": null
        },
        {
          "user": "lllllllllllll",
          "text": "Hi @yaspr, have you read the paper? you might want to take a look at their released code. github.com/s3team/uroboros",
          "time": null
        }
      ]
    },
    {
      "content": "By far, the established method for binary rewriting is dynamic rewriting where the binary is rewritten while being run on real inputs. Think of instrumentation tools like PIN, DynamoRIO and Dyninst and also binary translators like qemu. \n\nStatic rewriting tools have a fundamental challenge compared to dynamic rewriting which is precise Control Flow Graph recovery. That is, for each basic block in the binary we need to know the set of possible targets of its jump instruction. The difficulty is that binaries have many indirect jump instructions. For example, if we face a basic block that ends with bx r3 then we need to have a precise and reliable Value Set Analysis (VSA) that can tell us the possible values that r3 can take at run-time. Unfortunately, such analysis is, generally, undecidable. However, well-behaving compilers produce binaries that are structured somehow which is a fact that can be useful to a large extent. \n\nNote that the solving CFG recovery problem would allow us to solve the code/data separation problem as a by product. That is, recursive descent disassembly would allow us in that case to perfectly seperate code from data in the code byte stream.\n\nI can refer here to the following paper introduced in last year's USENIX Security:\n\nTheir tool  Uroboros is not open source. It's based on iterative linear sweep disassembly using objdump. The disassembly technique itself is discussed in an earlier paper. Nonetheless, it provides interesting techniques for static binary rewriting that actually works (or at least that is their claim). They even rewrite the same binary multiple times without breaking it. Finally, note that static binary rewriting is largely inapplicable to binaries with run-time code generation.\n\nUpdate:\n\nIt seems like many of the shortcomings of Uroboros has been addressed in  Ramblr which is discussed here:\n\nParticularly, they mention that their reassembled binaries have no execution overhead or size expansion.\n\n",
      "votes": "5",
      "user": "Codoka",
      "time": "May 22, 2017 at 9:43",
      "is_accepted": false,
      "comments": [
        {
          "user": "Trass3r",
          "text": "Ramblr seems to be abandoned. Alternative: github.com/GrammaTech/ddisasm",
          "time": null
        }
      ]
    },
    {
      "content": "Some systems use a simple executable format which is linked by combining the instructions and defined data contained in the processed assembly files, in a specified sequence; the linker will apply address fix-ups, but otherwise the contents of the output executable will be exactly what the programmer specified--nothing more; nothing less.  The old MS-DOS .COM file format was like that, and so it's possible for a disassembler to take a COM file and produce an file which, when assembled and linked, will yield a bit-identical file (the disassembly may have to use data directives for anything it doesn't understand, or for any instructions which might compile to something other than the bit pattern than appears in the file, but any executable-file bytes could be created using define-byte directives if nothing else.\n\nOther systems, however, use linkers which are more sophisticated and have to generate additional information which gets stored within the file.  It's possible that building the same source file with different versions of the tools may yield different binary files.  In general, when doing a disassemble-patch-reassemble sequence, one would strive to minimize the amount of code whose address changes.  If code stores the address of a function somewhere but the disassembler doesn't realize it's a function address rather than a constant, the only way the code will work is if the function stays at the same address.  Unfortunately, there's no way for an object file which is compatible with MS-DOS or Windows build tools to specify where things should be located.  When the code was originally compiled or assembled, the linker would be free to put the function anywhere and would update the \"constant\" function address appropriately.  When processing disassembled code, there's no way to ensure that the function stays at the old address, nor of having all addresses which depend upon it will get updated if it moves.\n\n",
      "votes": "4",
      "user": "supercat",
      "time": "Jun 24, 2014 at 20:11",
      "is_accepted": false,
      "comments": []
    }
  ]
}