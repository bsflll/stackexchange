{
  "title": "Floating point in RE intermediate languages like vine il, bap il, and google/zynamics reil",
  "link": "https://reverseengineering.stackexchange.com/questions/5998/floating-point-in-re-intermediate-languages-like-vine-il-bap-il-and-google-zyn",
  "content": "Are there any technical hurdles to implementing floating point support in re-oriented intermediate languages? I ask because none seem to support it, but give few reasons why. The only comment on the topic I've seen is from Sebastian Porst who in 2010 merely said\n\n",
  "votes": "2",
  "answers": 2,
  "views": "328",
  "tags": [
    "static-analysis"
  ],
  "user": "broadway",
  "time": "Jan 17, 2015 at 1:12",
  "comments": [],
  "answers_data": [
    {
      "content": "Floating point support is possible.  I think there are two reasons why it's not common:\n\n",
      "votes": "2",
      "user": "Ed McMan",
      "time": "Aug 1, 2014 at 22:55",
      "is_accepted": true,
      "comments": []
    },
    {
      "content": "There is an excellent recent work to translate floating point instructions to LLVM bitecode language, the project is called McSema and is managed by people at TrailOfBits.\n\nOne of the developer promised to get it OpenSource once the code will get in a good shape.\n\nEDIT: I just saw the answer from Ed McMan. I totally agree with him about the fact that the lack of tools handling this kind of problem makes it quite hard to integrate into binary program analysis framework. But, this is already a consequence of the problem, not a cause.\n\nIn fact, in my humble opinion, what is making this problem extremely tedious is its own nature. You have to deal with a continuous problem (logic on floating point numbers) and transform it into a discrete one (propositional logic).\n\nThe mix of these two models makes it very difficult to handle because a small difference in the input may end-up in a drastically different output (the bit-vector size may also have a big impact on the output). This kind of behavior is quite close to what you encounter in cryptographic hash functions, where a small modification of the input will result in a complete change of the output. \n\nAnd, this high variability of the output doesn't help tools to wrap all the behaviors into a meaningful logic formula that could be expressed in propositional logic along with the others.\n\nThere is maybe some hope if SMT-solvers start to consider mixing usual QF_AUFBV logic (often used for program simulation) and floating point logic (QF_LRA and QF_NRA).\n\n",
      "votes": "1",
      "user": "perror",
      "time": "Aug 2, 2014 at 8:05",
      "is_accepted": false,
      "comments": [
        {
          "user": "broadway",
          "text": "Yeah, I had forgotten about McSema even though I had seen the slides before.",
          "time": null
        },
        {
          "user": "broadway",
          "text": "Apparently mcsema was opensourced today. github.com/trailofbits/mcsema",
          "time": null
        },
        {
          "user": "Brendan Dolan-Gavitt",
          "text": "PANDA also supports lifting FPU operations to LLVM; it does so by using CLANG to compile the QEMU softfloat helper functions to LLVM bitcode, then linking that bitcode into execution at runtime.",
          "time": null
        }
      ]
    }
  ]
}