{
    "title": "Reverse engineering a proprietary GPU device driver?",
    "link": "https://reverseengineering.stackexchange.com/questions/8549/reverse-engineering-a-proprietary-gpu-device-driver",
    "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I have the binary file, and it's over 200 MB in size, which makes me think that such a task by myself would be beyond incredibly difficult to get anything done. I am trying to see how the driver communicates with the GPU exactly (because I need to emulate a GPU for a science project), and I need at least SOME IDEA of how DX and OpenGL work on the GPU's end from inspection of the device driver. It's 32 and 64 bit, and it works on Windows, but that's hardly a concern since the main care is in figuring out how exactly it sends data/command packets, what addresses they are sent to, how it scans PCI/etc., and how this can somewhat help me draw a better picture on how the GPU's internal functions work without any documentation. I know this will be hard, and I will manage to simulate how I think it will work, and only emulate how I know it is accessed, and work from there. I ask here because I need some help from the pros here on these subjects:</p>\n<p>1.How does one begin reverse engineering something of this tier?</p>\n<p>2.How much of the GPU's architecture should I expect to learn from this process?</p>\n<p>3.Will it be enough info overall to manage emulating/simulating the GPU somehow?</p>\n<p>I am 100% serious, so I hope for some answers that can at least set me in motion.</p>\n</div>",
    "votes": "5",
    "answers": 1,
    "views": "5k",
    "tags": [
        "binary-analysis",
        "driver"
    ],
    "user": "Truffles In Ma Bred",
    "time": "Mar 25, 2015 at 2:34",
    "comments": [
        {
            "user": "Guntram Blohm",
            "text": "<span class=\"comment-copy\">If you want to do this from start, you'll probably need to dedicate the next few years to the project. However, people have been reverse engineering ATI and NVidia cards to produce free Linux drivers, you might want to look at what those projects have done. <a href=\"http://nouveau.freedesktop.org/wiki/\" rel=\"nofollow noreferrer\">nouveau.freedesktop.org/wiki</a> and <a href=\"http://xorg.freedesktop.org/wiki/RadeonFeature\" rel=\"nofollow noreferrer\">xorg.freedesktop.org/wiki/RadeonFeature</a> might get you started. The drivers are not complete yet, so they may or may not have implemented the features you're interested in.</span>",
            "time": null
        },
        {
            "user": "rev",
            "text": "<span class=\"comment-copy\">this is kinda redundant, but make sure you're good at windows drivers development. reversing will be much easier then compared to if you start knowing nothing</span>",
            "time": null
        },
        {
            "user": "Truffles In Ma Bred",
            "text": "<span class=\"comment-copy\">@AcidShout It's not redundant if the end result is advanced GPU emulation that reverse engineering a device driver could help me accomplish. Most GPU emulation is very \"high-level,\" and doesn't deal with the dark and complex operations of the chip on a lower-level.</span>",
            "time": null
        },
        {
            "user": "wip",
            "text": "<span class=\"comment-copy\">Maybe this page can be of interest: \"Free and open-source graphics device driver\" (<a href=\"https://en.wikipedia.org/wiki/Free_and_open-source_graphics_device_driver\" rel=\"nofollow noreferrer\">en.wikipedia.org/wiki/â€¦</a>)</span>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>Hum ...\nReverse engineering such a large piece of software will require a lot of will, time, and, of course, technical knowledge. \nHonestly, unless you're considering starting a company or providing an open source/free software version of the driver, you'll be practically wasting your time. Especially if it is for a science project.</p>\n<p>You must keep in mind that reverse engineering is not an exact science, for that manufacturers always keep most of their technology under wraps.\nBut if you're motivated enough, you must start by knowing how the target technology works. This can be \"partially\" accomplished by reading the available manuals and micro-benchmarking the platform (in this case the GPU).\nI can assure you though, most architectural details provided in the manuals won't be 100% accurate. You'll always end up having outliers in your micro-benchmark results.</p>\n<p>Since you're more concerned by the GPU/Host communication protocol than the architectural aspect of the GPU, I would recommend you checking <a href=\"http://gpuocelot.gatech.edu/about/\" rel=\"nofollow\">Ocelot</a>.\nIt's an open source emulator that targets GPUs and CPUs, and I'm sure will be a great source of information.</p>\n<p>Know that if you're targeting nVidia's GPUs, the driver doesn't do much but load the <code>PTX</code> code in a specific memory area of the GPU where it'll be read from and executed. For more info about <code>PTX</code> go to this <a href=\"http://docs.nvidia.com/cuda/parallel-thread-execution/#axzz3VP4zXZBm\" rel=\"nofollow\">link</a>.  </p>\n</div>",
            "votes": "2",
            "user": "yaspr",
            "time": "Mar 25, 2015 at 13:28",
            "is_accepted": false,
            "comments": [
                {
                    "user": "Truffles In Ma Bred",
                    "text": "<span class=\"comment-copy\">Yeah, I've tried Ocelot before and it's not quite what I'm looking for. While it helps a bit, it doesn't solve the tough problems at all, such as knowing the GPU pipeline and instruction set in finer details. And unfortunately, CUDA isn't close enough to the metal: I expected to emulate a GPU to a point where an actual GPU device driver can interact with the emulated GPU on a bare metal system level. A lot of what you say helps, but that is in assuming I'm interested more in a compatibility layer or execution layer but not actually emulating a GPU like a CPU, down to the stack/register level.</span>",
                    "time": null
                },
                {
                    "user": "Truffles In Ma Bred",
                    "text": "<span class=\"comment-copy\">And yes, I wanted to make it open source and free if it proved worth it, based on how far I could get something going by myself. Basically, it will grow beyond me, but I want to get the wheels in motion first. Since a lot of GPU emulation is merely execution layer translations to host API functions, like OpenGL/DirectX, I wanted to tackle something I've never seen done before, which is system level GPU emulation from the ground up.</span>",
                    "time": null
                },
                {
                    "user": "yaspr",
                    "text": "<span class=\"comment-copy\">In that case, you're going to have to go through the architectural details of the targeted GPU, which are definitely not easy to find/figure out. Note that this will be specific to a certain hardware platform, and that it will be a huge mess to maintain and improve. I myself am responsible of hardware simulation for a well known microchip manufacturer, and I can assure you that even with access to confidential material it is hard to have great results. Bare metal is hell !</span>",
                    "time": null
                },
                {
                    "user": "Truffles In Ma Bred",
                    "text": "<span class=\"comment-copy\">Simulation of microchips? At what level? You can simulate, say, a GPU at a super high-level (like with emulation) where a known function packet can be translated to a direct API call for a texel/bitmap (OpenGL, DirectX), or you can take it a step further and (with emulation) quantify each step the GPU would take, more or less, so as replication of it as accurate as CPU emulation in emulators can get, more or less; or, instead, one could just emulate the GPU at a level only necessary for most internal operations (opcodes, memory, etc.) and every tiny detail could be skipped/high level emulated.</span>",
                    "time": null
                },
                {
                    "user": "yaspr",
                    "text": "<span class=\"comment-copy\">To put it simply, we simulate the hardware to a nanoscopic precision. It's mainly CPUs coupled with integrated GPUs ... and we use a supercomputer for that. I believe it's a way more advanced level than an emulation of opcodes and instructions. We usually cover all the hardware (interconnection network, instruction pipeline, ...) along with the high level constructs (opcodes, registers, ... ).</span>",
                    "time": null
                }
            ]
        }
    ]
}