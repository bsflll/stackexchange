{
    "title": "How does PeID calculate entropy?",
    "link": "https://reverseengineering.stackexchange.com/questions/9255/how-does-peid-calculate-entropy",
    "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I'm curious about the entropy field value that shows up when a binary is dropped into PeID. Does someone know how it arrives upon this value?</p>\n</div>",
    "votes": "1",
    "answers": 3,
    "views": "3k",
    "tags": [
        "obfuscation",
        "entropy"
    ],
    "user": null,
    "time": "Jun 29, 2015 at 18:02",
    "comments": [],
    "answers_data": [
        {
            "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>This is exactly a problem I dealt with several years ago because we also wanted to implement a close-as-possible solution for a project formerly based on PeID.</p>\n<p>Fortunately, in those days I found an old discussion in the PeID forum where someone asked the same question and got some hints from the author (not the exact code). I still have the link, but the entire site has ceased to exist :(</p>\n<p>Those days I managed to re-implement a version of that algorithm which yielded the same values most of the time. The core points of original PeID's entropy implementation are:</p>\n<ul>\n<li>It indeed uses the (Order-0) Shannon formula (see Guntrams hint) as core algorithm</li>\n<li>Such an entropy is calculated for each PE section independently</li>\n<li>use the raw size of each PE section (i.e. size on disk) as starting point for the data amount, then subtract all 0-Bytes at the very end from this value before calculating the Shannon entropy.</li>\n<li>sections may be ignored in the further calculation if following conditions apply:</li>\n<li><ul>\n<li>the section is the resource section</li>\n</ul></li>\n<li><ul>\n<li>no (remaining) data in the section (after null byte subtraction)</li>\n</ul></li>\n<li>the overall entropy is built by a weighted combination of all single section entropy values based on their (effective) lengths.</li>\n</ul>\n<p>This way you should be able to get very similar results. But expect deviations in non-standard samples (e.g. corruped/tampered PE files), i.e. you should expect and handle cases like implausible section sizes or files where all sections are excluded from the calculation...</p>\n<p>I hope this helps a bit and you can now successfully try this yourself... ;)</p>\n</div>",
            "votes": "4",
            "user": "Chris14",
            "time": "Jun 29, 2015 at 21:44",
            "is_accepted": false,
            "comments": [
                {
                    "user": "Jongware",
                    "text": "<span class=\"comment-copy\">Did you check the Wayback Machine for that page?</span>",
                    "time": null
                },
                {
                    "user": "Chris14",
                    "text": "<span class=\"comment-copy\">Yes, it's not indexed :(</span>",
                    "time": null
                }
            ]
        },
        {
            "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>You'd have to ask the creator of PeID which algorithm he uses.</p>\n<p>The standard formula for this is the <a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\" rel=\"nofollow\">Shannon formula</a>: H(x) = - sum i=1 to n p(i)*log2(p(i)).</p>\n<p><em>Why can't i use LaTeX here?</em></p>\n<p>An easy way to determine the entropy of a file is trying to compress it, using one of the standard compression Tools. Low entropies lead to better compression.</p>\n<p>Especially when you try to determine which sections are code and which are data, the compression test is a better indicator. The original shannon formula does not take into account if there are combinations of multiple bytes that occur often (like the push/pop sequences of function starts/ends), but compression algorithms recognize those.</p>\n</div>",
            "votes": "1",
            "user": "Guntram Blohm",
            "time": "Jun 29, 2015 at 20:51",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>a generic entropy calculation looks like this </p>\n<p>create a histogram of the contents of the binary \nmax 256 different bytes (0x0..0xff)<br/>\ncalculate (log base 2 of probability of each byte  multiplied by its count)\nadd them all up and divide by filesize</p>\n<pre>\n<h3><em>byte   count freq?        log<sub>2</sub></em> * count</h3>\n0x00     100   (100/filesize)     log<sub>2</sub>(freq[0]?)*count   \n0x01     045   (045/filesize)     log<sub>2</sub>(freq[1]?)*count    \n................................................    \n0xfe     020   (020/filesize)     log<sub>2</sub>(freq[0xfe]?)*count    \n0xff     180   (180/filesize)     log<sub>2</sub>(freq[0xfF]?)*count \n<h2>entropy =            ^sum(0x0..0xff)^/filesize       </h2></pre>\n<p>a sample code </p>\n<pre>\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\nint main (int argc,char *argv[]) {\n  if(argc !=2 ){ printf (\"usage %s file\",argv[0]);return 0;}\n  FILE * fp = 0;errno_t err = 0; unsigned long fsize = 0; float ent =0;\n  if (((err = fopen_s(&amp;fp,argv[1],\"rb\")) == 0)  &amp;&amp; (fp !=0)){\n    fseek(fp,0,SEEK_END);\n    fsize = ftell(fp);\n    fseek(fp,0,SEEK_SET);\n    unsigned char * buff = (unsigned char *)calloc(fsize,sizeof(char));\n    if(buff){\n      fread(buff,fsize,sizeof(char),fp);\n      fclose(fp);\n      unsigned long hist[256]= {0};\n      for(unsigned int i =0; i &lt; fsize; i++){\n        hist[buff[i]]++;        \n      }\n      for(unsigned int i=0;i&lt;256;i++){//log b2 (n) = loge(n)/loge(2)\n        ent+=((log((float)hist[i]/fsize)/log((float)2))*hist[i]);\n      }\n      printf(\"%f\\n\",-ent/fsize);\n      free(buff);\n    }    \n  }   \n}\n</pre>\n<p>result</p>\n<pre>\n:\\&gt;ls -l *\n-rw-rw-rw-  1 Admin 0 867 2015-06-30 03:30 entropy.cpp \n\n:\\&gt;..\\compile.bat    \n:\\&gt;if \"C:\\Program Files\\Microsoft Visual Studio 10.0\\VC\\\" == \"\" ()    \n:\\&gt;cl /Zi /EHsc /nologo /W4 /analyze *.cpp /link /RELEASE\nentropy.cpp  \n\n:\\&gt;ls -l *\n-rw-rw-rw-  1 Admin 0     867 2015-06-30 03:30 entropy.cpp\n-rwxrwxrwx  1 Admin 0  135680 2015-06-30 03:32 entropy.exe\n-rw-rw-rw-  1 Admin 0    5804 2015-06-30 03:32 entropy.obj\n-rw-rw-rw-  1 Admin 0 1297408 2015-06-30 03:32 entropy.pdb\n-rw-rw-rw-  1 Admin 0   53248 2015-06-30 03:32 vc100.pdb    \n\n<h2>:\\&gt;entropy.exe entropy.exe\n6.577609 </h2>\n</pre>\n<p>was tinkering with powershell and these two lines of script will calculate the same entropy as the c code above</p>\n<pre>\nPS &gt; cat .\\entropy.ps1\n$s=($f=gc -Encoding Byte $args)|measure;$g=$f|group -AsHashTable\n 0..255|%{ $e+=(((([math]::Log( $g[$_].count / $s.count)) /\n                    [math]::Log(2.0)) * $g[$_].count)/$s.count)};-$e\nPS &gt; .\\entropy.ps1 .\\entropy.exe\n<h1>6.57760952796101</h1>\n</pre>\n</div>",
            "votes": "1",
            "user": "blabb",
            "time": "Jun 30, 2015 at 20:03",
            "is_accepted": false,
            "comments": [
                {
                    "user": "mistika",
                    "text": "<span class=\"comment-copy\">There's a problem with such method. If you create a file that consists of 256 blocks, each one 256 bytes long and filled with the same byte value (256 zeros, 256 0x01s, .., 256 0xFFs),  it will give the result of 8.0 - the highest entropy, however such a file is very well compressible (64K become 420 bytes with Winrar).  This method doesn't count the distribution of bytes within a file.</span>",
                    "time": null
                }
            ]
        }
    ]
}