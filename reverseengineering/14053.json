{
    "title": "Is there a way to improve disassembler speed?",
    "link": "https://reverseengineering.stackexchange.com/questions/14053/is-there-a-way-to-improve-disassembler-speed",
    "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>I am currently disassembling quite a large file &gt;100MB. And I was wondering what factors reduce the amount of time needed to disassemble a large file completely.</p>\n<ul>\n<li><p>Is it a higher core count? </p></li>\n<li><p>Increased CPU frequency?</p></li>\n<li><p>More RAM? </p></li>\n<li><p>Faster IO reads and writes |SSD|M.2|?</p></li>\n</ul>\n</div>",
    "votes": "1",
    "answers": 1,
    "views": "5k",
    "tags": [
        "ida",
        "disassembly"
    ],
    "user": "user36278",
    "time": "Nov 27, 2016 at 7:05",
    "comments": [
        {
            "user": "w s",
            "text": "<span class=\"comment-copy\">Which tool are you using for disassembly ?</span>",
            "time": null
        },
        {
            "user": "user36278",
            "text": "<span class=\"comment-copy\">The tool I'm currently using is IDA v5 Free</span>",
            "time": null
        },
        {
            "user": "Jongware",
            "text": "<span class=\"comment-copy\">Perhaps you should <a href=\"https://reverseengineering.stackexchange.com/posts/14053/edit\">edit</a> that into your question – unless you are willing to switch to a faster tool.</span>",
            "time": null
        },
        {
            "user": "Jongware",
            "text": "<span class=\"comment-copy\">I must ask this: are you <i>sure</i> that your single 100MB is all code? The last time I saw something approaching that – an 80MB .exe file –, after some trivial checking turned out to be a mere unpacker of 0.5MB, and the rest was an entire folder of PKZIP compressed files.</span>",
            "time": null
        },
        {
            "user": "user36278",
            "text": "<span class=\"comment-copy\">@RadLexus  The program is about 80MB of code.The dissembled file is about 2.5 - 3.5 GB. As for the tools I used, I was not really concerned about the specifics, I just wanted to know what factors influence the speed of disassembly, which I believe yaspr answered beautifully.</span>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<div class=\"s-prose js-post-body\" itemprop=\"text\">\n<p>First, there are many algorithms used to disassemble a binary stream. The two most used ones are : <em>linear sweep</em> and <em>recursive traversal</em> (sometimes a hybrid), which present different performance and precision/reliability issues (how much code isn't disassembled properly : bad opcode, data interpreted as code, ...).<br/>\nSo before you tackle implementation related performance issues you should check the computational complexity of the disassembler you're using.</p>\n<p>Second, more RAM and faster I/O is - almost - always a performance enhancer. If you are disassembling a +100MB binary file, you'll need all the help you can from the hardware. Therefore, having a fast SSD could reduce the I/O overhead experienced with a traditional HDD. For RAM, the larger the size the better, but keep in mind that the differences in performance between DDR, DDR2, DDR3, ECC, ... are quite substantial. </p>\n<p>Now with regard to the CPU core count and frequency's effects on disassembly performance, it is pretty hard to evaluate. Why? Well, you must ask yourself the following set of questions :</p>\n<ol>\n<li>Does the disassembler you're using implement a parallel disassembly algorithm ?</li>\n<li>If yes, how does the performance of the disassembler relate/scale with the core count ? If no, then one core will suffice.</li>\n</ol>\n<p>FYI, a high core frequency (each core can have a frequency domain of its own separate from his neighbor's) could lead to disastrous performance if RAM is too slow. In the industry we usually settle for a core frequency of at most 1.8x the frequency of RAM. Otherwise, some serious performance bottlenecks (bandwidth, memory access latency, ...) start to tighten up and choke the running stream of instructions.</p>\n<p>From what I know, IDA doesn't perform any parallel disassembly - I might be wrong, always check. Therefore, the first two points you cited in your question don't really matter.</p>\n<p>The simplest way to answer your question would be to say that : the complexity of the disassembly algorithm and the way the implementation takes advantage of the underlying hardware are the key factors for performance. </p>\n<p>Edit :</p>\n<p><strong>[Bonus]</strong></p>\n<p>If you wish to program a parallel disassembler you can do so this way :</p>\n<ol>\n<li>Choose a disassembler library (Udis86 for example)</li>\n<li>Extract the sections and add them to a hashtab which contains a boolean variable - for each section - referring to its state (disassembled <strong>'1'</strong>, not disassembled <strong>'0'</strong>)</li>\n<li>Write a <em>threadable</em> function which task is to pick a section in the hashtab and disassemble it (you'll have to manage the sections order, ...). Once the task is done, update the state (you'll have to use a lock : a mutex) and pick another one if there are any left.</li>\n</ol>\n<p>This might look easy to do, but in fact it is a bit challenging for that it depends on the flexibility of the disassembly library. Honestly, +100MB binaries are quite rare and commercial/available tools aren't usually designed to handle that much code/data optimally. </p>\n<p>If you need any additional/more technical details let me know, I'll be glad to develop some points. </p>\n</div>",
            "votes": "4",
            "user": "yaspr",
            "time": "Nov 30, 2016 at 9:21",
            "is_accepted": true,
            "comments": []
        }
    ]
}