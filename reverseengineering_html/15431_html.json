{
    "title": "Automated Static Unpacking Binaries",
    "link": "https://reverseengineering.stackexchange.com/questions/15431/automated-static-unpacking-binaries",
    "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  I am reading the paper\n  <em>\n   Automatic Static Unpacking of Malware Binaries (Kevin Coogan et al.)\n  </em>\n  with the goal of trying to reproduce the given experimental results (with\n  <code>\n   Hybris-C\n  </code>\n  ,\n  <code>\n   MyDoom.q\n  </code>\n  ,\n  <code>\n   tElock\n  </code>\n  , etc), and studying how it can be expanded for other cases.\n </p>\n <p>\n  As far as I understand, the authors first use pointer analysis to extract the unpacking code by detecting\n  <em>\n   transition points\n  </em>\n  (i.e. the point separates the execution of the normal instructions from the runtime generated instructions), then the unpacker code is extracted using the backward slicing analysis from this point.\n </p>\n <p>\n  After a process of \"code punning\" and \"reassembly\" (e.g. patch out the defense code, fix some relocation problems, etc.), one could obtain a new binary whose each unpacked block is initially marked as a\n  <code>\n   s-object\n  </code>\n  (abbr. for\n  <em>\n   section objects\n  </em>\n  ). Then the new binary can be emulated (if I understood correctly) where\n  <code>\n   s-object\n  </code>\n  (s) will be filled out by unpacked code.\n </p>\n <p>\n  While I might be able to imagine some limits of static analysis applied in the paper (e.g. inaccuracies of backward static slicing, pointer analysis, side effects...), and the implicit hypothesis about the existence of the transition points, I still cannot figure out how the described static unpacker works.\n </p>\n <p>\n  <strong>\n   First\n  </strong>\n  , the authors say that the\n  <em>\n   backward static slicing\n  </em>\n  is applied since the context of the problem is unstructured binaries (and that is true since malicious codes are unstructured), but\n </p>\n <blockquote>\n  <p>\n   Problem 1: how can the\n   <em>\n    value-set analysis\n   </em>\n   be applied?\n  </p>\n </blockquote>\n <p>\n  since we have no hope to restore\n  <em>\n   abstract locations\n  </em>\n  (used by\n  <em>\n   value-set analysis\n  </em>\n  ) in unstructured binaries. For example, in the following unpacking stub of\n  <code>\n   Hybris\n  </code>\n  Worm:\n </p>\n <pre><code>    mov edx, 0x135\n    mov ebx, 0x401000\n    mov eax 0x6bf00803\n\nunpack:\n    sub [ebx], eax\n    nop\n    sub eax, 0x15e3c0\n    add ebx, 0x4\n    dec ecx\n    jne unpack\n    jmp _oep\n\n_oep:\n    ...\n</code></pre>\n <p>\n  I suppose that there would exist no abstract locations, no?\n </p>\n <p>\n  <strong>\n   Second\n  </strong>\n  , the authors say that each\n  <code>\n   s-object\n  </code>\n  contains meta-data (i.e. name, size, ...) about some section, as cited in\n  <code>\n   V.B.3.1\n  </code>\n  of the paper:\n </p>\n <blockquote>\n  <p>\n   an s-object... contains meta-data about the section it presents... These meta-data are obtained from section table of the binary...\n  </p>\n </blockquote>\n <p>\n  But\n </p>\n <blockquote>\n  <p>\n   Problem 2: how can we be sure that the unpacked code must be fit in a\n  section (whose info can be obtained by parsing the binary header)?\n  </p>\n </blockquote>\n <p>\n  That would be probably true in the case of \"pure\"\n  <code>\n   UPX\n  </code>\n  where the unpacked codes are located in the section\n  <code>\n   UPX0\n  </code>\n  , but this is not true in general (e.g. the case above of\n  <code>\n   Hybris\n  </code>\n  ).\n </p>\n <p>\n  Since there is a step of\n  <em>\n   address translation\n  </em>\n  described in\n  <code>\n   V.B.3.1\n  </code>\n  which arise from the difference between the normal runtime unpacking process of the binary and the unpacking process of the static unpacker, I assume that the output of this unpacker is a\n  <strong>\n   new binary\n  </strong>\n  contains only unpacked code and no unpacking stub. But\n </p>\n <blockquote>\n  <p>\n   Problem 3: how can it deal with multiple level packed programs?\n  </p>\n </blockquote>\n <p>\n  For example, the experimental results given in the\n  <code>\n   Figure 5\n  </code>\n  of the paper has the case of\n  <code>\n   Peed-44\n  </code>\n  which uses a customized\n  <code>\n   UPX\n  </code>\n  contains at least\n  <code>\n   2\n  </code>\n  unpacking levels: which form should be considered as the \"real\" unpacked code?\n </p>\n <p>\n  So my question is\n </p>\n <blockquote>\n  <p>\n   Is my understanding about the paper correct? (that should be not) then where did I misunderstand?\n  </p>\n </blockquote>\n</div>\n</body></html>",
    "votes": "8",
    "answers": 2,
    "views": "1k",
    "tags": [
        "malware",
        "static-analysis",
        "unpacking"
    ],
    "user": "Ta Thanh Dinh",
    "time": "May 26, 2017 at 15:21",
    "comments": [
        {
            "user": "NirIzr",
            "text": "<html><body><span class=\"comment-copy\">\n Without reading the paper, I'll comment that unfortunately, a lot of academic papers are oversimplified and unrealistic when it comes to real world examples in the security domain. Often times the tools are adjusted to handle the small subset of examples presented in the paper by overfitting the tools. This may be what you're experiencing.\n</span>\n</body></html>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  Regarding your first question:\n </p>\n <p>\n  If I understood the 3 papers necessary to answer the question correctly, your abstract locations would be\n  <code>\n   ebx\n  </code>\n  for example.\n </p>\n <p>\n  Now I hate academic papers with a passion but from glancing over your mentioned paper, the quoted paper for the used algorithm for Value-Set Analysis (\"What You See Is Not What You eXecute\") and in turn the paper quoted there to give an answer what Value-Set Analysis even is (\"Generic Value-Set Analysis on Low-Level Code\").\n </p>\n <p>\n  Basically, they track some slight abstraction of variables, which include registers (as explained in one of these papers, I don't want to go back and find out which one it was) over some code and observe how they change.\n </p>\n <p>\n  In your example of the Hybris worm,\n  <code>\n   ebx\n  </code>\n  would be an abstract location of interest, and Value-Set analysis would be used to observe what values\n  <code>\n   ebx\n  </code>\n  would assume over the loop code in order to deduce a potential location for unpacked code.\n </p>\n</div>\n</body></html>",
            "votes": "2",
            "user": "Johann Aydinbas",
            "time": "Apr 18, 2019 at 23:47",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  Regarding your 2nd question, usually you're not using sections but memory pages (and their associated permissions). You know the unpacked code will eventually have to be written then executed in a memory page, and you can easily detect such memory pages that have been writable, written to, and then executed.\n </p>\n <p>\n  Regarding your 3rd question, I'd say that a naive approach would be to run until the last stable \"transition point\" and then by definition, this last layer is the unpacked code. But more complex protections could break this heuristic very fast.\n </p>\n <p>\n  Regarding your first question, I'm not sure I understand any of those words.\n </p>\n</div>\n</body></html>",
            "votes": "1",
            "user": "user2823000",
            "time": "May 26, 2017 at 15:50",
            "is_accepted": false,
            "comments": []
        }
    ]
}