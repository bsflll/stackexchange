{
    "title": "Assembly Code - GCC optimized vs not",
    "link": "https://reverseengineering.stackexchange.com/questions/4011/assembly-code-gcc-optimized-vs-not",
    "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  I want to learn more about how GCC optimizes C programs. I have did a disas of a random function both optimized and unoptimized and I want to look at some of the differences. Off the top of my head, the optimized assembly has less jumps, and seems to use registers mostly, while the unoptimized is using memory more often. What other differences are there to note about these two?\n </p>\n <h2>\n  C code\n </h2>\n <pre><code>uint countPairsUpTo(int index, int* intArray, int first, int second)\n{\n  uint i;\n  uint sum = 0;\n\n  for (i = 0; i < index; i++)\n    if ((first == intArray[i]) && (second == intArray[i+2]))\n      sum++;\n\n  return sum ;\n}\n</code></pre>\n <h2>\n  Unoptimized\n </h2>\n <pre><code>0x080485b1 <countPairsUpTo+0>:  push   %ebp\n0x080485b2 <countPairsUpTo+1>:  mov    %esp,%ebp\n0x080485b4 <countPairsUpTo+3>:  sub    $0x10,%esp\n0x080485b7 <countPairsUpTo+6>:  call   0x8048418 <mcount@plt>\n0x080485bc <countPairsUpTo+11>: movl   $0x0,-0x4(%ebp)\n0x080485c3 <countPairsUpTo+18>: movl   $0x0,-0x8(%ebp)\n0x080485ca <countPairsUpTo+25>: jmp    0x80485fa <countPairsUpTo+73>\n0x080485cc <countPairsUpTo+27>: mov    -0x8(%ebp),%eax\n0x080485cf <countPairsUpTo+30>: shl    $0x2,%eax\n0x080485d2 <countPairsUpTo+33>: add    0xc(%ebp),%eax\n0x080485d5 <countPairsUpTo+36>: mov    (%eax),%eax\n0x080485d7 <countPairsUpTo+38>: cmp    0x10(%ebp),%eax\n0x080485da <countPairsUpTo+41>: jne    0x80485f6 <countPairsUpTo+69>\n0x080485dc <countPairsUpTo+43>: mov    0xc(%ebp),%edx\n0x080485df <countPairsUpTo+46>: add    $0x8,%edx\n0x080485e2 <countPairsUpTo+49>: mov    -0x8(%ebp),%eax\n0x080485e5 <countPairsUpTo+52>: shl    $0x2,%eax\n0x080485e8 <countPairsUpTo+55>: lea    (%edx,%eax,1),%eax\n0x080485eb <countPairsUpTo+58>: mov    (%eax),%eax\n0x080485ed <countPairsUpTo+60>: cmp    0x14(%ebp),%eax\n0x080485f0 <countPairsUpTo+63>: jne    0x80485f6 <countPairsUpTo+69>\n0x080485f2 <countPairsUpTo+65>: addl   $0x1,-0x4(%ebp)\n0x080485f6 <countPairsUpTo+69>: addl   $0x1,-0x8(%ebp)\n0x080485fa <countPairsUpTo+73>: mov    0x8(%ebp),%eax\n0x080485fd <countPairsUpTo+76>: cmp    -0x8(%ebp),%eax\n0x08048600 <countPairsUpTo+79>: ja     0x80485cc <countPairsUpTo+27>\n0x08048602 <countPairsUpTo+81>: mov    -0x4(%ebp),%eax\n0x08048605 <countPairsUpTo+84>: leave  \n0x08048606 <countPairsUpTo+85>: ret    \n</code></pre>\n <h2>\n  Optimized\n </h2>\n <pre><code>0x08048570 <countPairsUpTo+0>:  push   %ebp\n0x08048571 <countPairsUpTo+1>:  mov    %esp,%ebp\n0x08048573 <countPairsUpTo+3>:  push   %edi\n0x08048574 <countPairsUpTo+4>:  push   %esi\n0x08048575 <countPairsUpTo+5>:  push   %ebx\n0x08048576 <countPairsUpTo+6>:  call   0x8048418 <mcount@plt>\n0x0804857b <countPairsUpTo+11>: mov    0xc(%ebp),%ebx\n0x0804857e <countPairsUpTo+14>: mov    0x10(%ebp),%esi\n0x08048581 <countPairsUpTo+17>: mov    0x8(%ebp),%ecx\n0x08048584 <countPairsUpTo+20>: mov    $0x0,%edi\n0x08048589 <countPairsUpTo+25>: test   %ecx,%ecx\n0x0804858b <countPairsUpTo+27>: je     0x80485b2 <countPairsUpTo+66>\n0x0804858d <countPairsUpTo+29>: mov    $0x0,%edi\n0x08048592 <countPairsUpTo+34>: mov    $0x0,%edx\n0x08048597 <countPairsUpTo+39>: cmp    %esi,(%ebx,%edx,4)\n0x0804859a <countPairsUpTo+42>: jne    0x80485ab <countPairsUpTo+59>\n0x0804859c <countPairsUpTo+44>: mov    0x14(%ebp),%eax\n0x0804859f <countPairsUpTo+47>: cmp    %eax,0x8(%ebx,%edx,4)\n0x080485a3 <countPairsUpTo+51>: sete   %al\n0x080485a6 <countPairsUpTo+54>: movzbl %al,%eax\n0x080485a9 <countPairsUpTo+57>: add    %eax,%edi\n0x080485ab <countPairsUpTo+59>: add    $0x1,%edx\n0x080485ae <countPairsUpTo+62>: cmp    %ecx,%edx\n0x080485b0 <countPairsUpTo+64>: jne    0x8048597 <countPairsUpTo+39>\n0x080485b2 <countPairsUpTo+66>: mov    %edi,%eax\n0x080485b4 <countPairsUpTo+68>: pop    %ebx\n0x080485b5 <countPairsUpTo+69>: pop    %esi\n0x080485b6 <countPairsUpTo+70>: pop    %edi\n0x080485b7 <countPairsUpTo+71>: pop    %ebp\n0x080485b8 <countPairsUpTo+72>: ret    \n</code></pre>\n</div>\n</body></html>",
    "votes": "5",
    "answers": 2,
    "views": "2k",
    "tags": [
        "disassembly",
        "assembly"
    ],
    "user": "Terry Schmidt",
    "time": "Apr 3, 2014 at 7:48",
    "comments": [
        {
            "user": "DCoder",
            "text": "<html><body><span class=\"comment-copy\">\n \"Optimized\" can mean different things, depending on your goal - optimize for speed, optimize for size, optimize overall... And there are\n <i>\n  lots\n </i>\n of optimization options in GCC. Have you read\n <a href=\"http://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html\" rel=\"nofollow noreferrer\">\n  the documentation\n </a>\n about those options?\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "Terry Schmidt",
            "text": "<html><body><span class=\"comment-copy\">\n I compiled the optimized program with GCC with O1 optimization.  Does that help?\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "gandolf",
            "text": "<html><body><span class=\"comment-copy\">\n Like everyone mentioned, it could mean many different things, and what gets optimized out or 'in' would depend on how the compiler chooses to interpret the logic of the program. Here is a blog post of one particular feature of optimization called Dead Code Elimination or Code Motion (\n <a href=\"http://bangreverse.me/blog/?p=27\" rel=\"nofollow noreferrer\">\n  bangreverse.me/blog/?p=27\n </a>\n )\n</span>\n</body></html>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  Well, there are numerous optimization techniques performed by\n  <code>\n   GCC\n  </code>\n  . Such optimizations go from dead code elimination to loop unrolling, function inlining and many others. Before explaining the optimization techniques I'll start with what compilers do before optimizing.\n </p>\n <p>\n  Optimizations are usually performed on the\n  <code>\n   IR\n  </code>\n  (intermediate representation) of the provided code. Most compilers translate the input code (\n  <code>\n   C\n  </code>\n  ,\n  <code>\n   C++\n  </code>\n  ,\n  <code>\n   Fortran\n  </code>\n  , ...) into\n  <code>\n   IR\n  </code>\n  . This translation is performed by the\n  <code>\n   *front-end*\n  </code>\n  which feeds the\n  <code>\n   IR\n  </code>\n  to the\n  <code>\n   *middle-end*\n  </code>\n  that will apply optimization passes and feed it up to the\n  <code>\n   *back-end*\n  </code>\n  which will then generate machine code.\n </p>\n <p>\n  In\n  <code>\n   GCC\n  </code>\n  the IR is called GIMPLE and is presented briefly in\n  <a href=\"http://gcc.gnu.org/wiki/GIMPLE\" rel=\"nofollow\">\n   this\n  </a>\n  link. What\n  <code>\n   GCC\n  </code>\n  also does is convert its\n  <code>\n   GIMPLE\n  </code>\n  representation of the given code into\n  <code>\n   SSA\n  </code>\n  form (Static Single Assignment), which is described in\n  <a href=\"http://pages.cs.wisc.edu/~fischer/cs701.f08/ssa.pdf\" rel=\"nofollow\">\n   this\n  </a>\n  1989 publication.\n </p>\n <p>\n  If you follow\n  <a href=\"http://gcc.gnu.org/onlinedocs/gccint/Tree-SSA-passes.html#Tree-SSA-passes\" rel=\"nofollow\">\n   this\n  </a>\n  link you'll find all of the SSA optimization passes that\n  <code>\n   GCC\n  </code>\n  implements with a brief description. And I think you should also check the RTL passes.\n  <a href=\"ftp://ftp.uvsq.fr/pub/gcc/summit/\" rel=\"nofollow\">\n   Here\n  </a>\n  you'll find two directories full of detailed descriptions of what\n  <code>\n   GCC\n  </code>\n  exactly does. These are the most reliable references you can find since they are from the\n  <code>\n   GCC\n  </code>\n  people.\n </p>\n <p>\n  What you have to know is that some optimizations are performed multiple times and in a different order. For example, dead code elimination, which consists of eliminating islands from the\n  <code>\n   CFG\n  </code>\n  (Control Flow Graph) of the program, can be performed after certain optimizations which can cause islands creation.\n </p>\n <p>\n  Here's an example. Suppose you have a code that looks like this :\n </p>\n <pre><code>   int x = 1;\n\n   //Some code that doesn't alter the value of x\n\n   if (x == 2)\n    { BLOCK1; }\n   else\n      { BLOCK2; }\n</code></pre>\n <p>\n  Now, suppose that the compiler applies optimizations in this order :\n </p>\n <ol>\n  <li>\n   Dead code elimination\n  </li>\n  <li>\n   Branch prediction\n  </li>\n </ol>\n <p>\n  The first pass will, of course, find no islands in the\n  <code>\n   CFG\n  </code>\n  .\nThe second one (branch prediction), on the other hand, will figure out that the value of x doesn't change, and that it is different from 2 all the way from its declaration & initialization to its use in the if condition. This means that the whole if statement must be eliminated & replaced by BLOCK2. Thus a dead code elimination pass is necessary after branch prediction.\n </p>\n <p>\n  Another optimization is loop unrolling. It consists of duplicating the loop body and augmenting the stride in order to fill the CPU pipeline and have a better cache locality. For example, this\n  <em>\n   reduction\n  </em>\n  loop can be unrolled 4 times in order to gain some cycles and ameliorate cache access :\n </p>\n <pre><code>   //Ununrolled version\n   for (int i = 0; i < N; i++)\n       r += t[i];\n\n   //Unrolled version handling any value of N \n   for (int i = 0; i < (N & ~3); i += 4)\n     {\n        r += t[i]; \n        r += t[i + 1]; \n        r += t[i + 2]; \n        r += t[i + 3]; \n     }\n\n    //Handling the rest of the array elements \n    for (int i = (N & ~3); i < N; i++)\n        r += t[i];\n</code></pre>\n <p>\n  If we suppose that t is an array of floats (sizeof(float) = 4bytes) unrolling 4 times implies accessing 16bytes per iteration. If this code is run on an\n  <strong>\n   Intel\n  </strong>\n  <strong>\n   SandyBridge\n  </strong>\n  it will perform well, but not great (if prefetching isn't activated). Why ? Because a cache line size on the\n  <strong>\n   SandyBridge\n  </strong>\n  micro architecture is 64Bytes and the loop accesses 1/4 of a cache line per iteration. Unrolling 16 times will certainly result in much better performance.\n </p>\n <p>\n  The difficulty with this optimization technique is finding the right unroll factor for the right loop. Choosing a value too large or too small can result in performance drops. Also, the underlying architecture plays a very important role (the cache size on a\n  <strong>\n   Pentium\n  </strong>\n  and a\n  <strong>\n   Haswell\n  </strong>\n  isn't similar, same thing for the reorder buffers and pipelines). What some compilers do is perform static or dynamic analysis on a loop with different unroll factors and choose the one with the best profile.\n </p>\n <p>\n  These optimizations are usually performed on the IR but they can also be performed on assembly code. There are other optimizations that are tightly related to assembly, for example instructions reordering. This optimization consists of changing the order of instructions in order to have better instruction pipelining or parallel support. Sometimes, if a dependency exists between instructions, and if reordering induces great gains in performance, the dependency will be broken and the instructions reordered.\n  \n\n  The code below shows how reordering instructions can result in a much better construct :\n </p>\n <pre><code>   //Ordered\n   mov eax , [@1]\n   add eax , ecx\n   mov [@0], eax    \n   mul ecx , ebx\n   sub edx , eax\n\n   //Reordered \n   mov eax , [@1]\n   add eax , ecx\n   sub edx , eax\n   mul ecx , ebx\n   mov [@0], eax\n</code></pre>\n <p>\n  It is obvious that the ordered & reordered code perform the same operations, but the reordered version will consume less cycles than the ordered one because the\n  <em>\n   add\n  </em>\n  ,\n  <em>\n   sub\n  </em>\n  , and\n  <em>\n   mul\n  </em>\n  instructions, being of the same class, will not block one another in the pipeline. You can also notice that memory operations where put in the extremities of the code. This pattern usually results in a better performance than the ones with mixed up instructions, especially when such code is inside a loop or a basic block that is often executed.\n </p>\n <p>\n  There are plenty of interesting optimizations : auto-vectorization, function inlining, memory alignment, ... but unfortunately this page isn't large enough for me to cover them all. You can check\n  <code>\n   GCC\n  </code>\n  's source code and manuals for more information.\nThe references I pointed out above and below are quite helpful. If you manage to digest most of what they have to offer I'm sure you'll find what you are looking for.\n </p>\n <ul>\n  <li>\n   <a href=\"http://www.agner.org/optimize/\" rel=\"nofollow\">\n    http://www.agner.org/optimize/\n   </a>\n   \n\n  </li>\n  <li>\n   <a href=\"http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html\" rel=\"nofollow\">\n    http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html\n   </a>\n   \n\n  </li>\n  <li>\n   <a href=\"http://ukcatalogue.oup.com/product/9780198066644.do\" rel=\"nofollow\">\n    http://ukcatalogue.oup.com/product/9780198066644.do\n   </a>\n   \n\n  </li>\n </ul>\n</div>\n</body></html>",
            "votes": "6",
            "user": "yaspr",
            "time": "Jun 11, 2014 at 7:00",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  According to gcc manual,\n  <code>\n   -O1\n  </code>\n  that you mentioned in comments means turning on the following flags:\n </p>\n <pre><code>-fauto-inc-dec -fcprop-registers -fdce -fdefer-pop \n-fdelayed-branch -fdse -fguess-branch-probability \n-fif-conversion2 -fif-conversion -finline-small-functions \n-fipa-pure-const -fipa-reference -fmerge-constants \n-fsplit-wide-types -ftree-builtin-call-dce -ftree-ccp \n-ftree-ch -ftree-copyrename -ftree-dce -ftree-dominator-opts \n-ftree-dse -ftree-fre -ftree-sra -ftree-ter -funit-at-a-time\n</code></pre>\n <p>\n  You can read more about these flags at\n  <a href=\"http://linux.die.net/man/1/gcc\" rel=\"nofollow\">\n   gcc man page\n  </a>\n  .\nI would also recommend (If you didn't do that yet) to read\n  <a href=\"http://en.wikipedia.org/wiki/Dragon_Book\" rel=\"nofollow\">\n   Aho's Dragon books\n  </a>\n  .\n </p>\n</div>\n</body></html>",
            "votes": "5",
            "user": "perror",
            "time": "Jun 10, 2014 at 21:08",
            "is_accepted": false,
            "comments": []
        }
    ]
}