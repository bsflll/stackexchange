{
    "title": "Could a decompiler give better results if provided with part of the original code?",
    "link": "https://reverseengineering.stackexchange.com/questions/33207/could-a-decompiler-give-better-results-if-provided-with-part-of-the-original-cod",
    "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  I know it's generally impractical to decompile programs written in C/C++ and the like because most non-essential information is lost during the translation to machine code. Any decompiled code would have none of the original structure, no meaningful method and variable names, and definitely no comments.\n </p>\n <p>\n  However, if a compiler \"knows\" what part of the original code looked like, then could it produce more understandable code?\n </p>\n <p>\n  Suppose I have a program called\n  <code>\n   HelloWorld\n  </code>\n  that is built from the following three files:\n </p>\n <ul>\n  <li>\n   <code>\n    foo.c\n   </code>\n  </li>\n  <li>\n   <code>\n    bar.c\n   </code>\n  </li>\n  <li>\n   <code>\n    baz.c\n   </code>\n  </li>\n </ul>\n <p>\n  I want to decompile\n  <code>\n   HelloWorld\n  </code>\n  after having lost the source code. However, let's say I find an intact copy of\n  <code>\n   foo.c\n  </code>\n  backed up somewhere. If I could somehow \"feed\" that file to a decompiler, then could it reproduce the contents of\n  <code>\n   bar.c\n  </code>\n  and\n  <code>\n   baz.c\n  </code>\n  more accurately? If so, are there any decompilers that do this?\n </p>\n</div>\n</body></html>",
    "votes": "3",
    "answers": 1,
    "views": "113",
    "tags": [
        "decompilation"
    ],
    "user": "Danny",
    "time": "Sep 6, 2024 at 16:30",
    "comments": [
        {
            "user": "Rup",
            "text": "<html><body><span class=\"comment-copy\">\n IDA can import C header files, yes - realistically it’s the function signatures and type / structure information that are going to be most useful for the other files - and Ghidra has Parse C source. How well they work in practice I don’t know though.\n</span>\n</body></html>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  IMHO, the best you can do in the scenario you mention to improve the output of any of the major decompilers I know (Hex-Rays and Ghidra) is to import function names, prototypes, structs, enums, unions, defines, etc... to your binary project from the source files you have, so the output of the decompilers will be closer to the original sources. The process, however, won't be automatic almost 99% sure: I doubt you would be able to just tell IDA/Ghidra \"here are these source files, just parse them and import the definitions\". In my experience, almost always I have had to modify the header files, or generate one myself with the definitions I want, in order to be able to use the definitions in IDA/Ghidra.\n </p>\n <p>\n  Even if what I mentioned in the previous paragraph was possible to be done in an automated way, you will have the problem of how to match functions in the binary with functions in the source codes when there is no debugging information (in order to rename functions and apply function prototypes, etc). And even if debugging information is available, how to do so for non trivial C++ projects is... yet another whole project by its own. I have done some research (\n  <a href=\"https://github.com/joxeankoret/pigaios/blob/master/doc/Hacktivity_2018_presentation.pdf\" rel=\"nofollow noreferrer\">\n   Pigaios\n  </a>\n  ) on this area and all I can tell you is that it isn't any easy.\n </p>\n <p>\n  Other than that, I have seen some research projects trying to modify the output of working decompilers using LLMs telling them how to modify the output of the tool by feeding them multiple outputs of different decompilers and/or original source codes (like\n  <a href=\"https://github.com/radareorg/r2ai\" rel=\"nofollow noreferrer\">\n   r2ai\n  </a>\n  ). However, they are usually toy projects and, also, they have the inherent problem of using LLMs: decompilers must be as correct, accurate and exact as possible, and LLMs aren't, as they hallucinate stuff and there is no way to control this.\n </p>\n <p>\n  Another example, while not exactly the scenario you mention but it is however yet another method aiming to improve the readability of generated pseudo-code by training on real source codes: I have seen some papers, and a few tools, that trained models using Open Source software, to try to infer variable and function names in binaries using the trained model (like, for example,\n  <a href=\"https://files.sri.inf.ethz.ch/website/papers/ccs18-debin.pdf\" rel=\"nofollow noreferrer\">\n   Debin\n  </a>\n  ). The amount of false positives generated by such tools is astonishing.\n </p>\n <p>\n  So, in summary: the only realistic way of improving the output of decompilers as of today that I know of is to import, almost always manually, as much information as possible from source codes to your binary projects.\n </p>\n <p>\n  PS: Last but not least, you are partially wrong when you say \"Any decompiled code would have none of the original structure\" as there are some basic things preserved, like functions order in compilation units.\n </p>\n</div>\n</body></html>",
            "votes": "2",
            "user": "joxeankoret",
            "time": "Sep 8, 2024 at 10:51",
            "is_accepted": true,
            "comments": [
                {
                    "user": "Danny",
                    "text": "<span class=\"comment-copy\">Thanks for the detailed response. I've also wondered whether AI could be used in this manner. I've heard it's also possible to ask ChatGPT to explain what a block of code does, but my understanding is AI is still a long way from being able to make decompiled code more readable.</span>",
                    "time": null
                }
            ]
        }
    ]
}