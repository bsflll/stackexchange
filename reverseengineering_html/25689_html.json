{
    "title": "What browser do during initial web page request?",
    "link": "https://reverseengineering.stackexchange.com/questions/25689/what-browser-do-during-initial-web-page-request",
    "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  I am trying to understand why if I do fetch() 4 times on the active web page, after the fifth fetch server returns response that I am a \"robot\"\nI am using google chrome\nI go the following link:\n  <a href=\"https://ticketswestinw.evenue.net/cgi-bin/ncommerce3/SEGetEventInfo?ticketCode=GS%3ATWC%3AEARN20%3AEARN0807%3A&linkID=twcorp&shopperContext=&pc=&caller=&appCode=&groupCode=ARNBSB&cgc=&dataAccId=883&locale=en_US&siteId=ev_twcorp\" rel=\"nofollow noreferrer\">\n   tickets selling site\n  </a>\n  (might not open in non US countries). From the 'network' tab I 'copy as fetch' that very first request and paste it to the console and execute it\n </p>\n <p>\n  <a href=\"https://i.sstatic.net/3WwJ0.png\" rel=\"nofollow noreferrer\">\n   <img alt=\"copy the first page request from the network\" src=\"https://i.sstatic.net/3WwJ0.png\"/>\n  </a>\n </p>\n <p>\n  <a href=\"https://i.sstatic.net/K2t6Z.png\" rel=\"nofollow noreferrer\">\n   <img alt=\"enter image description here\" src=\"https://i.sstatic.net/K2t6Z.png\"/>\n  </a>\n </p>\n <p>\n  after doing so 5 times, I start receiving \"robots\" response\n </p>\n <p>\n  This is a good response:\n  <a href=\"https://i.sstatic.net/rcj7x.png\" rel=\"nofollow noreferrer\">\n   <img alt=\"enter image description here\" src=\"https://i.sstatic.net/rcj7x.png\"/>\n  </a>\n </p>\n <p>\n  this is a \"robot\" response:\n  <a href=\"https://i.sstatic.net/Edtca.png\" rel=\"nofollow noreferrer\">\n   <img alt=\"enter image description here\" src=\"https://i.sstatic.net/Edtca.png\"/>\n  </a>\n </p>\n <p>\n  And after that all requests made by the site itself return the same \"robots\" response everytime. But refreshing the page/opening new page, resets everything and I can send those requests again.\n </p>\n <p>\n  My question is: how can I understand what happens when I open browser page? What happens that the server starts returning good responses again after I refresh the page?\n </p>\n <p>\n  I was trying to drop cookies, but this didnt help.\n </p>\n <p>\n  The code I use to send request:\n </p>\n <pre><code>fetch(\"https://ticketswestinw.evenue.net/cgi-bin/ncommerce3/SEGetEventInfo?ticketCode=GS%3ATWC%3AEARN20%3AEARN0807%3A&linkID=twcorp&shopperContext=&pc=&caller=&appCode=&groupCode=ARNBSB&cgc=&dataAccId=883&locale=en_US&siteId=ev_twcorp\", {\n  \"headers\": {\n    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n    \"accept-language\": \"en-US,en;q=0.9\",\n    \"cache-control\": \"max-age=0\",\n    \"sec-fetch-dest\": \"document\",\n    \"sec-fetch-mode\": \"navigate\",\n    \"sec-fetch-site\": \"same-origin\",\n    \"sec-fetch-user\": \"?1\",\n    \"upgrade-insecure-requests\": \"1\"\n  },\n  \"referrer\": \"https://ticketswestinw.evenue.net/cgi-bin/ncommerce3/SEGetEventInfo?ticketCode=GS%3ATWC%3AEARN20%3AEARN0807%3A&linkID=twcorp&shopperContext=&pc=&caller=&appCode=&groupCode=ARNBSB&cgc=&dataAccId=883&locale=en_US&siteId=ev_twcorp\",\n  \"referrerPolicy\": \"no-referrer-when-downgrade\",\n  \"body\": null,\n  \"method\": \"GET\",\n  \"mode\": \"cors\",\n  \"credentials\": \"include\"\n}).then(x => x.text().then(y => console.log(y)));\n</code></pre>\n</div>\n</body></html>",
    "votes": "0",
    "answers": 1,
    "views": "64",
    "tags": [
        "javascript",
        "networking"
    ],
    "user": "simply good",
    "time": "Aug 18, 2020 at 14:39",
    "comments": [
        {
            "user": "Robert",
            "text": "<html><body><span class=\"comment-copy\">\n I would assume that they use JavaScript to generate something that changes in every request. If you replay existing requests 5 times it is recognized that the changing element was all the same which never happens if you use the web browser with JavaScript running.\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "simply good",
            "text": "<html><body><span class=\"comment-copy\">\n but I send that very initial request what is sent when I open the page, that one that gets HTML markup. So I assume that nothing can modify it, because the scripts are run after browser receives request response\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "Robert",
            "text": "<html><body><span class=\"comment-copy\">\n That long URL does not look like the start page. Also consider that you load the page but not the images, scripts, ... that are linked on it and that a regular web browser would load.\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "simply good",
            "text": "<html><body><span class=\"comment-copy\">\n This is exactly url that used for accessing the page, it contains event id and some additional info used in further requests >Also consider that you load the page but not the images, scripts Sorry I didnt get it, the response contains all the things like HTML + scripts + images\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "Robert",
            "text": "<html><body><span class=\"comment-copy\">\n Web browsers will download (while HTML requests is still running) all linked resources while your requests does not. This difference can be detected on server side -> robot detected. Together with TLS fingerprinting and a lot of other facts that indicator if you use  a web browser or not.\n</span>\n</body></html>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  There are two major differences between a web page loaded via web browser and via script:\n </p>\n <ol>\n  <li>\n   <p>\n    While loading the web page a web browser also loads linked resources like scripts, images, css files, ... This can be recognized on server side.\n   </p>\n  </li>\n  <li>\n   <p>\n    A web browser not just downloads the web page but also executed the contained JavaScript. From within JavaScript additional resources can be loaded or special calculations can be performed and the result can be sent back to the web server.\n   </p>\n  </li>\n  <li>\n   <p>\n    For web pages that are loaded via HTTPS there are additional values (TLS extensions, the list of available TLS version(s) and cipher suites, ...) in the TLS protocol that allow to perform some sort of web browser fingerprinting on TLS level.\n   </p>\n  </li>\n  <li>\n   <p>\n    Last but not least there are scripts such as Google Recaptcha v3 available that use JavaScript for detecting web-browsers and human interaction.\n   </p>\n  </li>\n </ol>\n <p>\n  All those measures together can be combined to perform a robot/script detection on server side.\n </p>\n</div>\n</body></html>",
            "votes": "1",
            "user": "Robert",
            "time": "Aug 19, 2020 at 7:12",
            "is_accepted": false,
            "comments": []
        }
    ]
}