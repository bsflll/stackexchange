{
    "title": "What is an entropy graph",
    "link": "https://reverseengineering.stackexchange.com/questions/21555/what-is-an-entropy-graph",
    "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  I am new to reversing and I see a tool\n  <a href=\"https://github.com/horsicq/Detect-It-Easy\" rel=\"noreferrer\">\n   Detect It Easy\n  </a>\n  and it has a feature called\n  <em>\n   Entropy\n  </em>\n  . I want to know what it is used for?\n </p>\n</div>\n</body></html>",
    "votes": "5",
    "answers": 5,
    "views": "13k",
    "tags": [
        "entropy"
    ],
    "user": "Suman Mandal",
    "time": "Jun 27, 2019 at 11:56",
    "comments": [],
    "answers_data": [
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <blockquote>\n  <p>\n   it has a feature called Entropy. I want to know what it is used for?\n  </p>\n </blockquote>\n <p>\n  For our purposes, entropy can be though of as information density or as a measure of randomness in information, which is what makes it useful in the context of reverse engineering and binary analysis.\n </p>\n <p>\n  Compressed and encrypted data have higher entropy than e.g. code or text data. In fact, compressed and encrypted data have close to the maximum possible level of entropy, which can be used as a heuristic to identify it as such in order to differentiate it from non-compressed/non-encrypted data.\n </p>\n <p>\n  Example use cases in reverse engineering:\n </p>\n <ul>\n  <li>\n   <p>\n    <strong>\n     Malware Analysis\n    </strong>\n    - If we have an executable which has a header that can be parsed successfully and the program loads and runs without error, but the overall entropy level of the file is very high and the code can't be analyzed statically because the data outside of the file header and program headers looks random (hence the high entropy), it probably means that the executable is in fact compressed on disk and is decompressed at runtime. Executable compression complicates analysis, so it is a relatively common feature of programs developed for criminal purposes. If we want to analyze the code, its decompressed form need to be recovered somehow.\n   </p>\n  </li>\n  <li>\n   <p>\n    <strong>\n     Firmware Analysis\n    </strong>\n    - In systems with relatively severe hardware constraints, such as embedded systems, firmware updates are often delivered in compressed form in order to save space. In order to analyse the firmware, it first needs to be determined whether it is encrypted or compressed. One way to determine this is through performing an entropy analysis of the file. If the entropy is very high, it is a good sign that the file is indeed compressed or encrypted. To proceed with analysis of the actual firmware, it must first be decompressed/decrypted. If we have a block of data with very high entropy (i.e. close to random), it makes no sense to try to treat it as code and disassemble it, because the results will be meaningless nonsense.\n   </p>\n  </li>\n  <li>\n   <p>\n    <strong>\n     File Type Identification\n    </strong>\n    - Some file types can be identified on the basis of their overall entropy. For example, we can usually differentiate between image files (png, jpeg, etc) and compiled binaries (ELF, PE) because image files consist of compressed data and therefore (generally) have much higher entropy than compiled binaries.\n   </p>\n  </li>\n </ul>\n <p>\n  Besides \"Detect It Easy\", tools such as\n  <code>\n   binwalk\n  </code>\n  ,\n  <code>\n   ent\n  </code>\n  and binvis.io can assist with calculating file entropy. You can also build your own tools that do this.\n </p>\n</div>\n</body></html>",
            "votes": "3",
            "user": "julian",
            "time": "Jun 27, 2019 at 14:10",
            "is_accepted": true,
            "comments": []
        },
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  Entropy is interpreted as the Degree of Disorder or Randomness\n </p>\n <p>\n  a high entropy means a highly disordered set of data\n </p>\n <p>\n  a low entropy means an ordered set of data\n </p>\n <p>\n  to address the comments\n  \n\n  order here does not mean 'a' following 'a' kind of order  it is to be interpreted as random / non random state of certain data\n </p>\n <p>\n  <strong>\n   aaaabbbbccccdddd\n  </strong>\n  or\n  <strong>\n   \"abcdabcdabcdabcd\"\n  </strong>\n  or\n  <strong>\n   \"adbcadbcadbcadbc\"\n  </strong>\n  is a repetitive string whose entropy will be greater than\n  \n\n  <strong>\n   aaaaaaaabbbbcccd\n  </strong>\n  or any shuffled representation of this string\n </p>\n <p>\n  in the first string and its shuffled clones all have 4 chars with equal probability\n  <strong>\n   4/16 or 1/4 or 25%\n  </strong>\n  \n\n  but in the second string char\n  <strong>\n   'a'  (8/16 ) or half of the data set\n  </strong>\n  has the highest probability\n  \n\n  while\n  <strong>\n   'c' (1/16) has the least\n  </strong>\n  or a very minuscule probability\n </p>\n <p>\n  entropy is a thermodynamic concept that was introduced to digital science (information theory) \nas a means to calculate how random a set of data is\n </p>\n <p>\n  simply put the highest compressed data will have the highest entropy\n </p>\n <p>\n  where all the 255 possible bytes will have equal frequencies\n </p>\n <p>\n  ie if 0x00 was seen 10 times in a blob \n0x10 or 0x80 or 0xff will all be seen 10 times in the same blob\n </p>\n <p>\n  that is the blob will be a repeated sequence comprising of all bytes between of 0x0..0xff\n </p>\n <p>\n  while a low entropy blob will have a repeated sequence comprising only of a certain byte like  0x00  0r  0x55  or two bytes 0x0d0a  ox222e etc or any series one less than 255  possible byte sequences\n </p>\n <p>\n  taking an algo from\n  <a href=\"https://stackoverflow.com/questions/15450192/fastest-way-to-compute-entropy-in-python\">\n   here\n  </a>\n  and modifying it a little\n </p>\n <pre><code>import math\nfrom collections import Counter\nbase = {\n    'shannon' : 2.,\n    'natural' : math.exp(1),\n    'hartley' : 10.,\n    'somrand'   : 256.\n    }\ndef eta(data, unit):\n    if len(data) <= 1:\n        return 0\n    counts = Counter()\n    for d in data:\n        counts[d] += 1\n        ent = 0\n        probs = [float(c) / len(data) for c in counts.values()]\n        for p in probs:\n            if p > 0.:\n                ent -= p * math.log(p, base[unit])\n    return ent\nhes = \"abcde\\x80\\x90\\xff\\xfe\\xde\"\nles = \"aaaaa\\x61\\x61\\x61\\x61\\x61\"\nprint (\"=======================================================================================================\")\nprint (\" type      ent for hes                 hes                      ent for les            les\")\nprint (\"=======================================================================================================\")\nfor i in base:\n    for j in range(1,4,1):\n        print (i ,' ', eta( j*hes,i) , '\\t', (hes*j + (30 -j *10) *\" \" ) , ' ' , eta (j*les , i) ,'\\t',  (\"%s\" % les*j )) \n</code></pre>\n <p>\n  you can see  'abcde\\x80.....' is high entropy while 'aaaaa\\x61...' is low entropy\n </p>\n <pre><code>:\\>python foo.py\n=======================================================================================================\n type      ent for hes                 hes               ent for les            les\n=======================================================================================================\nshannon   3.321928094887362      abcdeÿþÞ                   0.0    aaaaaaaaaa\nshannon   3.321928094887362      abcdeÿþÞabcdeÿþÞ           0.0    aaaaaaaaaaaaaaaaaaaa\nshannon   3.321928094887362      abcdeÿþÞabcdeÿþÞabcdeÿþÞ   0.0    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nnatural   2.3025850929940455     abcdeÿþÞ                   0.0    aaaaaaaaaa\nnatural   2.3025850929940455     abcdeÿþÞabcdeÿþÞ           0.0    aaaaaaaaaaaaaaaaaaaa\nnatural   2.3025850929940455     abcdeÿþÞabcdeÿþÞabcdeÿþÞ   0.0    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nhartley   0.9999999999999998     abcdeÿþÞ                   0.0    aaaaaaaaaa\nhartley   0.9999999999999998     abcdeÿþÞabcdeÿþÞ           0.0    aaaaaaaaaaaaaaaaaaaa\nhartley   0.9999999999999998     abcdeÿþÞabcdeÿþÞabcdeÿþÞ   0.0    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nsomrand   0.4152410118609203     abcdeÿþÞ                   0.0    aaaaaaaaaa\nsomrand   0.4152410118609203     abcdeÿþÞabcdeÿþÞ           0.0    aaaaaaaaaaaaaaaaaaaa\nsomrand   0.4152410118609203     abcdeÿþÞabcdeÿþÞabcdeÿþÞ   0.0    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n</code></pre>\n</div>\n</body></html>",
            "votes": "5",
            "user": "blabb",
            "time": "Jun 27, 2019 at 17:59",
            "is_accepted": false,
            "comments": [
                {
                    "user": "julian",
                    "text": "<span class=\"comment-copy\">\"a high entropy means a highly disordered set of data  a low entropy means an ordered set of data\" &lt;- This is a false statement. Order is not relevant, because entropy is calculated over a distribution where each value in that distribution has a probability associated with it. Compressed and encrypted data have high entropy because the probability associated with each byte value in the distribution is roughly equal (the distribution of byte values in the data is close to uniform), not because of the order the byte values appear in the bytestream.</span>",
                    "time": null
                },
                {
                    "user": "julian",
                    "text": "<span class=\"comment-copy\">\"where all the 255 possible bytes will have equal frequencies\" &lt;- You probably meant \"where all byte values between 0 and 255 (256 total) have an equal probability in the overall distribution\" (the frequency of each byte value between 0-255 is the same in the distribution).</span>",
                    "time": null
                },
                {
                    "user": "blabb",
                    "text": "<span class=\"comment-copy\">@julian what do you mean by order ? like a follows a b =&gt; b kind of order ?  order in my answer does not mean a sorted / sequential / non sequential data  i meant it as in an orderly / non random state the most random repetitive  data where the count of each value tends to be equal has the highest entropy   it may be a (military type ordered / sorted set like aaaabbbbccccdddd  4[a,b,c,d]  but this will tend to have an entropy greater than aaaaaaaabbbbbccd   8[a],4[b],2[c],1[d]  here  is a theory link using hard technical words <a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\" rel=\"nofollow noreferrer\">en.wikipedia.org/wiki/Entropy_(information_theory)</a></span>",
                    "time": null
                },
                {
                    "user": "julian",
                    "text": "<span class=\"comment-copy\">You said “ordered set”, so maybe I misunderstood your meaning. Anyway, your main point about the relationship between entropy and randomness is correct.</span>",
                    "time": null
                },
                {
                    "user": "perror",
                    "text": "<span class=\"comment-copy\">@julian: What are you saying ??? I do think that blabb is correct. You seems to be stuck on only one definition of entropy, but they are two... which are both perfectly valid. So, stop yelling at everyone please.</span>",
                    "time": null
                }
            ]
        },
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  Just to add (small) piece of information to @blabb and @Johann Aydinbas answers, here is the cite from\n  <em>\n   Practical Malware Analysis\n  </em>\n  book regarding your question:\n </p>\n <blockquote>\n  <p>\n   Packed executables can also be detected via a technique known as entropy\n  calculation. Entropy is a measure of the disorder in a system or program [...]\n  </p>\n  <p>\n   Compressed or encrypted data more closely resembles random data,\n  and therefore has high entropy; executables that are not encrypted or compressed have lower entropy. Automated tools for detecting packed programs often use heuristics like entropy.\n  </p>\n </blockquote>\n <p>\n  You can find additional information\n  <a href=\"http://n10info.blogspot.com/2014/06/entropy-and-distinctive-signs-of-packed.html\" rel=\"nofollow noreferrer\">\n   here\n  </a>\n  , under\n  <strong>\n   Increased entropy\n  </strong>\n  header.\n </p>\n</div>\n</body></html>",
            "votes": "4",
            "user": "bart1e",
            "time": "Jun 27, 2019 at 8:30",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  Shannon's entropy comes from information theory. It is the measure of degree of randomness of text. If a string has greater Shannon's entropy it means it's a strong password. Principally, Shannon entropy equation provides a way to predict the average minimum number of bits required to encode a string of symbols, based on the frequency of the symbols.\n </p>\n <p>\n  <a href=\"https://i.sstatic.net/OuGIB.png\" rel=\"nofollow noreferrer\">\n   <img alt=\"Formula for base 2\" src=\"https://i.sstatic.net/OuGIB.png\"/>\n  </a>\n </p>\n <p>\n  Note that the base represents the number of possible characters. Base 2 can be replaced by any base. As can be seen in this code where it's replaced by 255.\n </p>\n <p>\n  This link has a simplest implementation of the algorithm for calculating entropy of novels and religious books. It tells us a lot. For example, that all the human generated books have nearly identical degree of fluctuation between disorder. It is a good feature of data.\nThis is the link to code mentioned above.\n  <a href=\"https://github.com/QuantumNovice/ShanonEntropyOfBooks\" rel=\"nofollow noreferrer\">\n   Information Entropy of different Books\n  </a>\n </p>\n</div>\n</body></html>",
            "votes": "3",
            "user": "Holomorphic Guy",
            "time": "Jun 27, 2019 at 14:09",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  First, you have to know that the term\n  <em>\n   entropy\n  </em>\n  is used to refer to two different concepts which are somehow related if you think twice, but as it is really not obvious at first sight, you should prefer to consider these two as different concepts.\n </p>\n <h2>\n  Defining Entropy ?\n </h2>\n <p>\n  The entropy that you want to know about can be defined as\n  <a href=\"https://en.wikipedia.org/wiki/Entropy_(order_and_disorder)\" rel=\"nofollow noreferrer\">\n   the amount of order, disorder, or chaos in a thermodynamic system\n  </a>\n  .\n </p>\n <p>\n  On the other hand, the\n  <em>\n   other\n  </em>\n  entropy is coming from information theory and can be seen as\n  <a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\" rel=\"nofollow noreferrer\">\n   a measure of the amount of information that can be stored in a system\n  </a>\n  .\n </p>\n <h1>\n  Why is it useful in RE ?\n </h1>\n <p>\n  An entropy graph (to evaluate the amount of disorder) can be useful to detect the parts of the file that get close to random data. It will allow to detect the parts that have been encrypted/compressed and the parts that appear to be left untouched.\n </p>\n <p>\n  Indeed, a high disorder in data is exactly what you want to achieve when encrypting data. And, I told you that the two entropy definitions were related, if you store a lot of information in a minimum of bytes, it appears to be with a high level of disorder, so is compression...\n </p>\n <p>\n  That is why we use entropy graphs of files, be able to distinguish raw parts from encrypted/compressed sub-parts without any prior information of the file format.\n </p>\n <h1>\n  An Example\n </h1>\n <p>\n  For example, here is an entropy graph from the tool\n  <code>\n   binwalk\n  </code>\n  coming from\n  <a href=\"https://reverseengineering.stackexchange.com/questions/16939/decompress-and-analyse-vmware-efi64-bios\">\n   another question from here\n  </a>\n  :\n </p>\n <p>\n  <a href=\"https://i.sstatic.net/dtSg2.png\" rel=\"nofollow noreferrer\">\n   <img alt=\"Binwalk entropy graph\" src=\"https://i.sstatic.net/dtSg2.png\"/>\n  </a>\n </p>\n <p>\n  Directly from this graph we can see that there is a first part that appear to be raw (probably asm opcodes if we look at the shape of the curve), then a part which is much likely encrypted (compression does not reach an entropy of 1 with such regularity usually), and finally padding with always the same byte (e.g.\n  <code>\n   0x00\n  </code>\n  or\n  <code>\n   0xff\n  </code>\n  ).\n </p>\n</div>\n</body></html>",
            "votes": "0",
            "user": "Community",
            "time": "Jun 17, 2020 at 9:54",
            "is_accepted": false,
            "comments": [
                {
                    "user": "julian",
                    "text": "<span class=\"comment-copy\">\"The entropy that you want to know about can be defined as the amount of order, disorder, or chaos in a thermodynamic system.\" &lt;- This is a false statement. Software is not a thermodynamic system and does not possesses the property \"energy\" (heat). Software consists of encoded information, therefore to measure its properties - such as its <a href=\"https://crypto.stackexchange.com/questions/10404/estimating-bits-of-entropy\">Shannon entropy</a> - the tools provided by information theory are appropriate.</span>",
                    "time": null
                },
                {
                    "user": "julian",
                    "text": "<span class=\"comment-copy\">If you don't believe me, please examine <a href=\"https://github.com/ReFirmLabs/binwalk/blob/35f7fa0fc2be42a4259b0ed9e5a8cca1c54764f5/src/binwalk/modules/entropy.py#L221\" rel=\"nofollow noreferrer\">the code that was used to generate the entropy plot</a> in your post. Binwalk calculates information entropy level in terms of either zlib compression ratio or Shannon entropy.</span>",
                    "time": null
                },
                {
                    "user": "julian",
                    "text": "<span class=\"comment-copy\">\"Your definition of entropy is perfectly valid if you are considering information theory. But, unfortunately, the entropy referred here is much likely the one coming from thermodynamics (i.e. the degree of disorder).\" &lt;- The meaning seems quite clear.</span>",
                    "time": null
                },
                {
                    "user": "julian",
                    "text": "<span class=\"comment-copy\">it’s nothing personal. It’s just not correct.</span>",
                    "time": null
                },
                {
                    "user": "perror",
                    "text": "<span class=\"comment-copy\">It looks personal, just by the way you are harassing me with that.</span>",
                    "time": null
                }
            ]
        }
    ]
}