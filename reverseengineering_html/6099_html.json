{
    "title": "Unknown compression algorithm",
    "link": "https://reverseengineering.stackexchange.com/questions/6099/unknown-compression-algorithm",
    "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  I got data which is compressed but I fail to find the compression algorithm.\nThe data is part of a larger file from which I know the layout, So I managed to find out few things. What I know:\n </p>\n <ul>\n  <li>\n   I don't have the binary executable that load the data, I only have the updated version which no longer support the old copression algo. I tortured it in many way and it just doesn't contain the corresponding code\n  </li>\n  <li>\n   it is compressed (100% sure of it)\n  </li>\n  <li>\n   it can be home made as it was replaced later (see below)\n  </li>\n  <li>\n   no magic numbers so far\n  </li>\n  <li>\n   it is not plain:\n   <ul>\n    <li>\n     deflate (wrong header)\n    </li>\n    <li>\n     lzma (wrong header)\n    </li>\n    <li>\n     gzip (wrong header)\n    </li>\n    <li>\n     Quantum (wrong header)\n    </li>\n    <li>\n     Microsoft CAB (wrong header)\n    </li>\n    <li>\n     Bzip2 (wrong header)\n    </li>\n    <li>\n     Zip (wrong header)\n    </li>\n   </ul>\n  </li>\n  <li>\n   the uncompressed size is given in the file containing the data, this file layout is fully reversed and does not contain any clue\n  </li>\n  <li>\n   it might be encrypted but is unlikely because of speed requirements\n  </li>\n  <li>\n   if it is encrypted, it gives the same output given the same data input at the beginning of the sequence (by guess on some uncompressed data located nearby)\n  </li>\n  <li>\n   it is from 2001 and has been replaced by deflate since\n  </li>\n  <li>\n   Some of those data only output ASCII and nothing else (I know it from the layout of the container file) and have a compression ratio of about 0.30  (compressedSize/uncompressedSize) everytime\n  </li>\n  <li>\n   I don't have any before/after data sadly\n  </li>\n </ul>\n <p>\n  EDIT: There are the 32 first bytes in hex:\nb9daed36cb64bedb61b9dd2cb72afd8ee565b0dd2ea00f0afda2c36eb25b0016\n </p>\n <p>\n  I made histograms of several of those data and they all match a specific pattern. Something is going on with the powers of 2 obviously but I fail to see what.\n </p>\n <p>\n  <a href=\"https://i.sstatic.net/MRK5H.jpg\" rel=\"noreferrer\">\n   <img src=\"https://i.sstatic.net/bq9jG.png\" title=\"Hosted by imgur.com\"/>\n  </a>\n </p>\n <p>\n  Anyone has a idea what it could be? What can I do to gather further information? Does it look Lempel-Ziv based? If yes how could I reverse it?\n </p>\n</div>\n</body></html>",
    "votes": "5",
    "answers": 1,
    "views": "4k",
    "tags": [
        "file-format"
    ],
    "user": "search4everNever",
    "time": "Aug 19, 2014 at 14:28",
    "comments": [
        {
            "user": "Igor Skochinsky",
            "text": "<html><body><span class=\"comment-copy\">\n If you don't have the executable how did you get this file? Also, show a dump of the 16-32 bytes of the header.\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "search4everNever",
            "text": "<html><body><span class=\"comment-copy\">\n Updated to answer + hex dump of the header\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "perror",
            "text": "<html><body><span class=\"comment-copy\">\n You may find this question and answers interesting to look at: >\n <a href=\"https://reverseengineering.stackexchange.com/questions/1463/are-there-any-tools-or-scripts-for-identifying-compression-algorithms-in-executa\">\n  Are there any tools or scripts for identifying compression algorithms in executables?\n </a>\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "Jongware",
            "text": "<html><body><span class=\"comment-copy\">\n \"The output is ASCII and nothing else\" - that, coupled with a ratio of \"about 0.30\" actually suggests a\n <i>\n  simple\n </i>\n compression scheme, rather than a complicated: a dedicated text-only compression scheme. Can we see this file?\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "Jason Geffner",
            "text": "<html><body><span class=\"comment-copy\">\n Have you tried contacting the company that made the software?\n</span>\n</body></html>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <ul>\n  <li>\n   First of all, you forgot lzo. AFAIR it was invented in 1996.\n  </li>\n  <li>\n   Second of all, it is possible that the compressed data has no corresponding standard header (which makes all \"incorrect header\" mistakes not necessarily true).\n  </li>\n  <li>\n   Third of all, the histogram doesn't look like histogram of data compressed as one piece, it is possible that it has some internal structure (may be blobs with sizes before them ?).\n  </li>\n </ul>\n <p>\n  I'd suggest to write script that runs different decompression algorithms on parts of the data and to observe results.\n </p>\n</div>\n</body></html>",
            "votes": "3",
            "user": "w s",
            "time": "Aug 20, 2014 at 7:09",
            "is_accepted": false,
            "comments": []
        }
    ]
}