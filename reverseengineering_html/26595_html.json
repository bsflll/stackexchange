{
    "title": "Shannon Entropy of Individual PE sections",
    "link": "https://reverseengineering.stackexchange.com/questions/26595/shannon-entropy-of-individual-pe-sections",
    "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  Why is Shannon Entropy of individual sections always between 0-8. Also why we need to create a 256 freq array while calculating the Shannon Entropy?\n </p>\n</div>\n</body></html>",
    "votes": "2",
    "answers": 1,
    "views": "85",
    "tags": [
        "malware"
    ],
    "user": "Rishabh Jain",
    "time": "Dec 20, 2020 at 20:12",
    "comments": [
        {
            "user": "David",
            "text": "<html><body><span class=\"comment-copy\">\n The mathematical answer is here:\n <a href=\"https://stats.stackexchange.com/questions/95261/why-am-i-getting-information-entropy-greater-than-1\" title=\"why am i getting information entropy greater than 1\">\n  stats.stackexchange.com/questions/95261/…\n </a>\n</span>\n</body></html>",
            "time": null
        },
        {
            "user": "David",
            "text": "<html><body><span class=\"comment-copy\">\n An intuitive answer is here, which also answers your question about the frequency array (hint: each byte represents one of 256 discrete states):\n <a href=\"https://towardsdatascience.com/the-intuition-behind-shannons-entropy-e74820fe9800\" rel=\"nofollow noreferrer\">\n  towardsdatascience.com/…\n </a>\n</span>\n</body></html>",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "<html><body><div class=\"s-prose js-post-body\" itemprop=\"text\">\n <p>\n  The channel capacity of a single byte samples has a maximum of 8 bits.\n </p>\n <p>\n  Another way of thinking about it: If a single byte has the same value for every sample, you need 0 additional bits of information to describe the values.\n </p>\n <p>\n  If a single byte takes on 256 different values, [0 to 255], then for every sample you will need 8 additional bits of information to uniquely describe the values.\n </p>\n <p>\n  For example, if you were measuring the Shannon Entropy of a collection of Short values (2 bytes / 16 bits) it would range from 0 (constant) to 16 (completely random).\n </p>\n</div>\n</body></html>",
            "votes": "2",
            "user": "pythonpython",
            "time": "Dec 21, 2020 at 15:44",
            "is_accepted": true,
            "comments": []
        }
    ]
}