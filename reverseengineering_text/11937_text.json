{
    "title": "Is it possible to programmatically determine if a crash resulted from a buffer overflow?",
    "link": "https://reverseengineering.stackexchange.com/questions/11937/is-it-possible-to-programmatically-determine-if-a-crash-resulted-from-a-buffer-o",
    "content": "Crash analysis: \nGiven a large set of crashes, all resulting from access violations, is it possible to (with or without access to symbols and source code) programmatically determine whether the crashes resulted from a bad read from crashes caused by a bad write? My ultimate goal is distinguish buffer overruns from other crashing conditions. \nEven more specifically, I would like to be able to separate corruptions of EIP (or similar instruction pointers in other architectures) from other forms of crashes so that I might better separate buffer overflows from a corpus of crashing input files. \nAdditionally:\nWhat tool would you use to instrument this or otherwise automate this process? \nI suspect that this is not possible since the ultimate source of the crash will be dereferencing a bad address in all cases. Walking back, maybe it is possible to mark all memory allocated to buffers and check whether the crashing address is within a reasonable offset to a buffer. Perhaps I am missing a much simpler way of checking.  \nFor the record I am doing this on a userland program in a *nix environment. A general purpose answer would be awesome, but I would also be happy to see an array of platform specific answers. \nEdit: Don't be afraid to move this to SecuritySE or elsewhere if you think it doesn't belong on RE.\n",
    "votes": "0",
    "answers": 1,
    "views": "242",
    "tags": [
        "debugging",
        "instrumentation",
        "fuzzing"
    ],
    "user": "MrSynAckSter",
    "time": "Feb 4, 2016 at 23:25",
    "comments": [
        {
            "user": "user2823000",
            "text": "Don't you have this information in the stack trace already? Use eip to get your faulty instruction and check if it's a read or a write given the value of the registers.\n",
            "time": null
        },
        {
            "user": "MrSynAckSter",
            "text": "I want something automated that will do this for me over thousands of crashes. Also, I imagine that many things could happen between the invalid pointer being set, and it being dereferenced.\n",
            "time": null
        },
        {
            "user": "user2823000",
            "text": "What are you calling \"crashes\"? Do you have the binaries or just dump/traces? In both case, it should be scriptable without problem. Distinguishing read crashes from write crashes is easy. But unless I'm missing something, I don't see how that would help you separate out-of-bound write from other crashing conditions. Or you'll have to specify what you mean by a \"bad read\" and a \"bad write\" if it's not trying to access a memory address you cannot read, resp. write.\n",
            "time": null
        },
        {
            "user": "MrSynAckSter",
            "text": "I have binaries which crash the application, but I could get crash dumps pretty easily. I want to detect buffer overflows (which I think usually crash when EIP sends execution into invalid memory and would be caused by WRITING into the wrong address) and other crashes (which might be caused by doing any other operation in invalid memory, like maybe mov ESP, 0x0000001 - which crashes but doesn't help me control EIP)\n",
            "time": null
        },
        {
            "user": "user2823000",
            "text": "Oh, so you're trying to separate \"EIP corruption\" crashes from the other ones. I'd reformulate your question to emphasize this point.\n",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "it is not clear if you are using windbg and you have a bunch of .dmp files that you want to auto analyze for access-violation if that is the case you could check this out \noriginal contents of directory\n<pre><code>:\\>ls -l\ntotal 96\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 CRASH.DMP\n</code></pre>\nduplicating the same file 10 times to simulate a bunch of crash dumps\n<pre><code>:\\>for /L %I IN (1,1,10) do copy crash.dmp %I.dmp\n\n:\\>copy crash.dmp 1.dmp\n        1 file(s) copied.\n\n--------------\n</code></pre>\ndirectory contents post duplication\n<pre><code>:\\>ls -l\ntotal 1056\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 1.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 10.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 2.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 3.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 4.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 5.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 6.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 7.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 8.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 9.dmp\n-rw-rw-rw-  1 HP 0 96698 2016-01-29 21:27 CRASH.DMP\n</code></pre>\nrunning cdb with no symbol path to eliminate symbol loading and running a command on each of the file and grepping the result \n<pre><code>:\\>for /F %I In ('dir /b') Do cdb -sins -z %I -c \".exr -1;q\" | grep -i att\n</code></pre>\nresult as follows\n<pre><code>:\\>cdb -sins -z 1.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 10.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 2.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 3.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 4.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 5.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 6.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 7.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 8.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z 9.dmp -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n\n:\\>cdb -sins -z CRASH.DMP -c \".exr -1;q\"   | grep -i att\nAttempt to read from address 0a29f6ac\n</code></pre>\nby programmaticlally if you mean writing a tool to open and check each dump \nyes than it is possible to write a standalone exe using the dbgeng function\n",
            "votes": "3",
            "user": "blabb",
            "time": "Feb 4, 2016 at 6:01",
            "is_accepted": false,
            "comments": [
                {
                    "user": "MrSynAckSter",
                    "text": "<span class=\"comment-copy\">Whoah this is coo! I can't accept for a couple of reasons, though: 1) I am on Unix here 2) I am debugging userland applications. Thanks though! If nothing else comes up I will accept this.</span>",
                    "time": null
                },
                {
                    "user": "MrSynAckSter",
                    "text": "<span class=\"comment-copy\">Never mind didn't realize what you were doing in your answer, was late. I have crashing examples, not dumps. Still, this looks to be a Windows KD strategy. I wonder if I can port the strategy to GDB with crash dumps.</span>",
                    "time": null
                }
            ]
        }
    ]
}