{
    "title": "What browser do during initial web page request?",
    "link": "https://reverseengineering.stackexchange.com/questions/25689/what-browser-do-during-initial-web-page-request",
    "content": "I am trying to understand why if I do fetch() 4 times on the active web page, after the fifth fetch server returns response that I am a \"robot\"\nI am using google chrome\nI go the following link: tickets selling site (might not open in non US countries). From the 'network' tab I 'copy as fetch' that very first request and paste it to the console and execute it\n\n\nafter doing so 5 times, I start receiving \"robots\" response\nThis is a good response:\n\nthis is a \"robot\" response:\n\nAnd after that all requests made by the site itself return the same \"robots\" response everytime. But refreshing the page/opening new page, resets everything and I can send those requests again.\nMy question is: how can I understand what happens when I open browser page? What happens that the server starts returning good responses again after I refresh the page?\nI was trying to drop cookies, but this didnt help.\nThe code I use to send request:\n<pre><code>fetch(\"https://ticketswestinw.evenue.net/cgi-bin/ncommerce3/SEGetEventInfo?ticketCode=GS%3ATWC%3AEARN20%3AEARN0807%3A&linkID=twcorp&shopperContext=&pc=&caller=&appCode=&groupCode=ARNBSB&cgc=&dataAccId=883&locale=en_US&siteId=ev_twcorp\", {\n  \"headers\": {\n    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n    \"accept-language\": \"en-US,en;q=0.9\",\n    \"cache-control\": \"max-age=0\",\n    \"sec-fetch-dest\": \"document\",\n    \"sec-fetch-mode\": \"navigate\",\n    \"sec-fetch-site\": \"same-origin\",\n    \"sec-fetch-user\": \"?1\",\n    \"upgrade-insecure-requests\": \"1\"\n  },\n  \"referrer\": \"https://ticketswestinw.evenue.net/cgi-bin/ncommerce3/SEGetEventInfo?ticketCode=GS%3ATWC%3AEARN20%3AEARN0807%3A&linkID=twcorp&shopperContext=&pc=&caller=&appCode=&groupCode=ARNBSB&cgc=&dataAccId=883&locale=en_US&siteId=ev_twcorp\",\n  \"referrerPolicy\": \"no-referrer-when-downgrade\",\n  \"body\": null,\n  \"method\": \"GET\",\n  \"mode\": \"cors\",\n  \"credentials\": \"include\"\n}).then(x => x.text().then(y => console.log(y)));\n</code></pre>\n",
    "votes": "0",
    "answers": 1,
    "views": "64",
    "tags": [
        "javascript",
        "networking"
    ],
    "user": "simply good",
    "time": "Aug 18, 2020 at 14:39",
    "comments": [
        {
            "user": "Robert",
            "text": "I would assume that they use JavaScript to generate something that changes in every request. If you replay existing requests 5 times it is recognized that the changing element was all the same which never happens if you use the web browser with JavaScript running.\n",
            "time": null
        },
        {
            "user": "simply good",
            "text": "but I send that very initial request what is sent when I open the page, that one that gets HTML markup. So I assume that nothing can modify it, because the scripts are run after browser receives request response\n",
            "time": null
        },
        {
            "user": "Robert",
            "text": "That long URL does not look like the start page. Also consider that you load the page but not the images, scripts, ... that are linked on it and that a regular web browser would load.\n",
            "time": null
        },
        {
            "user": "simply good",
            "text": "This is exactly url that used for accessing the page, it contains event id and some additional info used in further requests >Also consider that you load the page but not the images, scripts Sorry I didnt get it, the response contains all the things like HTML + scripts + images\n",
            "time": null
        },
        {
            "user": "Robert",
            "text": "Web browsers will download (while HTML requests is still running) all linked resources while your requests does not. This difference can be detected on server side -> robot detected. Together with TLS fingerprinting and a lot of other facts that indicator if you use  a web browser or not.\n",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "There are two major differences between a web page loaded via web browser and via script:\n\nWhile loading the web page a web browser also loads linked resources like scripts, images, css files, ... This can be recognized on server side.\n\nA web browser not just downloads the web page but also executed the contained JavaScript. From within JavaScript additional resources can be loaded or special calculations can be performed and the result can be sent back to the web server.\n\nFor web pages that are loaded via HTTPS there are additional values (TLS extensions, the list of available TLS version(s) and cipher suites, ...) in the TLS protocol that allow to perform some sort of web browser fingerprinting on TLS level.\n\nLast but not least there are scripts such as Google Recaptcha v3 available that use JavaScript for detecting web-browsers and human interaction.\n\n\nAll those measures together can be combined to perform a robot/script detection on server side.\n",
            "votes": "1",
            "user": "Robert",
            "time": "Aug 19, 2020 at 7:12",
            "is_accepted": false,
            "comments": []
        }
    ]
}