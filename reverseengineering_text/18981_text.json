{
    "title": "Why are these 0-byte jumps used as a delay?",
    "link": "https://reverseengineering.stackexchange.com/questions/18981/why-are-these-0-byte-jumps-used-as-a-delay",
    "content": "I'm debugging an old PC BIOS and see a lot of what I assume are delay sequences, using 0-byte jump instructions (<pre><code>EB 00</code></pre> = <pre><code>jmp short $+2</code></pre>).\n\nWhy this particular instruction? I'm guessing it must have some desirable timing properties.\nThe CPU in question is a 386SX clocked at 16MHz.\n",
    "votes": "2",
    "answers": 1,
    "views": "168",
    "tags": [
        "disassembly",
        "x86",
        "bios"
    ],
    "user": "pesco",
    "time": "Aug 5, 2018 at 10:59",
    "comments": [],
    "answers_data": [
        {
            "content": "Here's a fragment from the leaked AWARD BIOS source code (file <pre><code>COMMON.MAC</code></pre>):\n<pre><code>SIODELAY    MACRO               ; SHORT IODELAY\n        jmp short $+2\n        ENDM\n\nIODELAY     MACRO               ; NORMAL IODELAY\n        siodelay\n        siodelay\n        ENDM\n\nWAFORIO     MACRO               ; NORMAL IODELAY\n        siodelay\n        siodelay\n        siodelay\n        siodelay\n        siodelay\n        siodelay\n        ENDM\n\nNEWIODELAY      MACRO\n        out 0ebh,al              \n            ENDM  \n</code></pre>\nSo apparently this was intended specifically as a delay after I/O operations, presumably to give the potentially slow hardware time to process the request from the CPU.\nI also found this in the OS/2 programming FAQ:\n\nQuestion: I looked at some code in ddk\\src\\dev\\mouse\\bus.asm which diddles with the interrupt controller, and it calls MyIODelay in\n  between IN and OUT instructions. I'm not clear why these are required.\n  It says something about letting the bus catch up - is this brain dead\n  hardware or what? \nAnswer: You were right first time - brain dead hardware. \"Some\" (not all) IO devices cannot do: \n<pre><code>  in al,dx\n  or al,MASK\n  out dx,al\n</code></pre>\nas the CPU is faster than the IO peripherals, received wisdom is that\n  you should put an arbitrary small delay between IO access to the same\n  IO device. And most people use: \n<pre><code>  in al,dx\n  or al,MASK\n     jmp next_instruction\nnext_instruction:\n  out dx,al\n</code></pre>\nThe jump op has the additional \"benefit\" of flushing the CPUs\n  instruction pipeline and thereby slowing down processing even more.\n\nIt's not quite the pattern in your snippet so could be just an instance of cargo-cult copypasting.\n",
            "votes": "3",
            "user": "Igor Skochinsky",
            "time": "Aug 5, 2018 at 18:55",
            "is_accepted": true,
            "comments": [
                {
                    "user": "pesco",
                    "text": "<span class=\"comment-copy\">Cool, thanks, that helps. The part about flushing the pipeline actually confirms what I suspected. Though I still wonder how the proper reasoning would go, rather than \"everybody does this and it seems to work\".</span>",
                    "time": null
                },
                {
                    "user": "Igor Skochinsky",
                    "text": "<span class=\"comment-copy\">you could try asking on retrocomputing, maybe people there know more about early hardware</span>",
                    "time": null
                },
                {
                    "user": "gilm",
                    "text": "<span class=\"comment-copy\">I remember that I used to nop my way through and that worked fine for me. My only guess is that jmp flushes cache, plus, it takes many more clock ticks per bytes that just a single nop. They probably want to save ROM bytes yet still have lots of cycles spared. It only seems a mystery because you know there are better ways than jmp+$2 :)</span>",
                    "time": null
                }
            ]
        }
    ]
}