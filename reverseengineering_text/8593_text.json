{
    "title": "Unknown decompression algorithm",
    "link": "https://reverseengineering.stackexchange.com/questions/8593/unknown-decompression-algorithm",
    "content": "I work on the reverse engineering of the ChessBase archive (.cbv).\nI found the general structure of the file and can already decompress some files.\nYou can see my current work here.\nHowever, some .cbv files that are bigger seems to use a second compression algorithm.\nI was able to find the first compression algorithm by debugging the ChessBase Reader 2013 software, but I cannot make sense of the second compression algorithm.\nI tried some tools like <pre><code>signsrch</code></pre> to find out what algorithm was used without any luck: it seems to be a custom algorithm.\nHere is a file that I am able to partly decompress with my tool (my tool will print <pre><code>What to do?</code></pre> when it detects that the unknown compression algorith was used).\nDo you have any idea of what compression algorithm is used?\nIf not, do you have any way of finding it by looking at the compressed file?\nI am able to create archives so I can have files that are both compressed and not: I wonder if there is any way to find a compression pattern in such a situation.\n",
    "votes": "7",
    "answers": 1,
    "views": "2k",
    "tags": [
        "decompress"
    ],
    "user": "antoyo",
    "time": "Mar 29, 2015 at 17:10",
    "comments": [],
    "answers_data": [
        {
            "content": "After downloading ChessBase Reader and playing with ProcMon a bit to find the function that reads the archive and writes the data file, i loaded up the whole thing in IDA to analyze it. The data is Huffman-coded. \nEach data block has the following structure. Note that Huffman compression works with bits, not bytes, so each size in the following table is in bits as well. The block length is 16 bits, or 2 bytes, for example.\n<pre><code>+----------------------------------------------+\n|                                              |\n|16 bits - uncompressed block length (len)     |\n|                                              |\n+-----------+----------------------------------+\n|           |                                  |\n|Repeat     |  4 bits - length of entry (n)    |\n|256        |                                  |\n|times      +----------------------------------+\n|           |                                  |\n|one entry  |  n bits - tree left/right        |\n|per byte   |  information for this byte       |\n|(0-255)    |                                  |\n|           |                                  |\n+-----------+----------------------------------+\n|                                              |\n| Huffman encoded bit sequences. The number of |\n| bits isn't stored anywhere, but the number   |\n| of sequences, which is equal to the number   |\n| of output bytes, is the block length (len)   |\n|                                              |\n+----------------------------------------------+\n</code></pre>\nAssuming the word \"foobar\" was coded in this scheme, this would possibly  result in (i made up the bit values for the characters):\n<pre><code>+----------------+\n|Huffman code for|\n|character   is  |\n+--------+-------+\n|        |       |\n| o      |   0   |\n| f      |   100 |\n| b      |   101 |\n| a      |   110 |\n| r      |   111 |\n|        |       |\n+--------+-------+\n</code></pre>\nThis would result in the word foobar being coded as\n<pre><code>100 0 0 101 110 111</code></pre>. The length is 6 bytes, or <pre><code>0000 0000 0000 0110</code></pre> in 16 bits.\nThe bitarray for <pre><code>foobar</code></pre>, formatted to the above table, would read\n<pre><code>0000 0000 0000 0110                      (16 bit output length)\n.....    array index   0 for byte '\\0'\n.....    array index   1 for byte '\\1'\n.....\n0011 110 array index  97 for byte 'a'    (3 bits)\n0011 101 array index  98 for byte 'b'    (3 bits)\n.....\n0011 100 array index 102 for byte 'f'    (3 bits)\n.....\n0001 0   array index 111 for byte 'o'    (1 bit)\n.....\n0011 101 array index 114 for byte 'r'    (3 bits)\n.....                                    remaining bit combos - 255\n\n100 0 0 101 110 111                      foobar text\n</code></pre>\nThe implementation builds a binary tree from the code table. When it reads the data, it starts at the root of the tree; each bit moves down the tree, to the left or right, depending on the next bit value. When a leaf is reached, the corresponding byte is being output. This repeats until the length of the output stream is reached.\nThe related functions from the binary are these:\n<pre><code>BECAA0</code></pre>: decodes the archive data. Reads 16 bits for the length; then reads the encoding table into two arrays at offsets <pre><code>080A</code></pre> (bits) and <pre><code>0E10</code></pre> (bit lengths) within the decoder class. After this, call <pre><code>BEC930</code></pre> to decode the data bytes.\n<pre><code>BEBF30</code></pre>: One parameter (number of bits), gets this many bits from the input array. At the end of the function, the word at offset <pre><code>1014</code></pre> has these bits.\n<pre><code>BEBAD0</code></pre>: Builds the tree from the arrays at <pre><code>080A</code></pre> and <pre><code>0E10</code></pre>\n<pre><code>BEC930</code></pre>: Calls <pre><code>BEBAD0</code></pre> to build the tree, then reads the remaining bits from the input stream. Walks the tree for each bit; emits a byte when a leaf is found. At the end, calls <pre><code>BEBA90</code></pre> to destroy the tree.\n<pre><code>BEBA90</code></pre>: Recursively delete a node by deleting the left and right children, the the node itself.\nI don't think debugging the writer would be easier if you want to read the files; compression has a lot of logic and data structures, and knowing how 'one way' works doesn't neccesarily help you with the other way round. In this case, luckily, its a well known algorithm, but if the algorithm is unknown it can be quite hard to compress effectively if you just know how to decompress.\n",
            "votes": "8",
            "user": "Guntram Blohm",
            "time": "Mar 31, 2015 at 10:08",
            "is_accepted": true,
            "comments": [
                {
                    "user": "broadway",
                    "text": "<span class=\"comment-copy\">database encryption is certainly an option. See <a href=\"http://www.chesscentral.com/basic_chess_database_a/300.htm\" rel=\"nofollow noreferrer\">chesscentral.com/basic_chess_database_a/300.htm</a> so it's possible that's what you're looking at.</span>",
                    "time": null
                },
                {
                    "user": "Guntram Blohm",
                    "text": "<span class=\"comment-copy\">@broadway Thank you for pointing this out. Howeverm in the meantime, i've massaged IDA a bit more; i'm quite sure this is a huffman decompression algorithm, although i don't have everything worked out yet.</span>",
                    "time": null
                },
                {
                    "user": "Jongware",
                    "text": "<span class=\"comment-copy\">Looking at the last few kilobytes, you can see 'normal text' fragments interspersed with binary codes. Definitely a Huffman variant.</span>",
                    "time": null
                },
                {
                    "user": "antoyo",
                    "text": "<span class=\"comment-copy\">Thanks for your answer: I'll look at this. @Jongware: I already know the compression algorithm used when we see normal text fragments (see my tool on github). My issue is when there is no text fragments. By the way, would it be easier to figure out the algorithm by debugging the application that creates such archives (ChessBase)?</span>",
                    "time": null
                }
            ]
        }
    ]
}