{
    "title": "Are there any metrics for code obfuscation?",
    "link": "https://reverseengineering.stackexchange.com/questions/186/are-there-any-metrics-for-code-obfuscation",
    "content": "Is source code obfuscation quantifiable ?\nAre there any metrics that measure the degree of obfuscation in a source file ?\n",
    "votes": "16",
    "answers": 3,
    "views": "1k",
    "tags": [
        "obfuscation"
    ],
    "user": "asheeshr",
    "time": "Mar 23, 2013 at 14:19",
    "comments": [],
    "answers_data": [
        {
            "content": "For source code, one possibility is McCabe Cyclomatic Complexity. Some source code analysis tools (McCabe IQ) will use this as a measure of \"crappy\" code that you should investigate and re-write. Cyclomatic complexity generally indicates excessively convoluted or complicated program logic, and code that will be hard to understand. It might not reflect actual difficulty to reverse engineer though, and also doesn't take into account other types of obfuscations such as constant variable encryption and code encryption, etc. \nI think it would be very difficult to objectively quantify all types of obfuscations, because you are trying to measure their ability to withstand attack by an adversary. If you can reduce your program protection problems to cryptographically complex problems (as the authors of Gauss were able to) then I think that is a useful obfuscation metric as it represents work the attacker must perform to un-do your obfuscation. Otherwise, the work level is so variable from obfuscation to obfuscation across one reverse engineer to another that I'm not sure it can be meaningfully measured...\n",
            "votes": "11",
            "user": "Andrew",
            "time": "Mar 23, 2013 at 15:32",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "There is a purely theoretical metric for code obfuscation based on abstract interpretation which is delightful.  It relies upon the fact that abstract interpretations are comparable with respect to their precision.  In particular, in the upper closure operator formulation of abstract interpretation -- where an upper closure operator is created by composing the abstraction and concretization functions, forming a map from the concrete domain to itself -- we can compare the potency of two abstract interpretations by partially-ordering the set of fixedpoints of a given closure map.  An operator with strictly more fixedpoints is more precise than, say, one that maps everything to top.\nIn order to compare obfuscations, we consider the set of abstract interpretations over the original version of the program, and then also over the obfuscated version.  Now we can use the same construction described above to compare the potency of obfuscating transformations.  Given some observable property a, as encoded by an abstract interpretation, we say that the obfuscator is potent for a if, when preserving abstract interpretation with respect to that property, the fixedpoint is different for the original version versus the transformed version.\nRead more here:  http://profs.sci.univr.it/~dallapre/ICALP05.pdf\n",
            "votes": "10",
            "user": "0xC0000022L",
            "time": "Jan 9, 2023 at 8:40",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "To be more practical in the question of code obfuscation, there are several metrics that can be used:\n\nThe percentage of optimized and non-optimized obfuscated code after the compiler optimization pass and the overall effectiveness of the obfuscation.\nThe percentage of changed bytes at the binary level for two obfuscated binaries, as well as the percentage of similarity between them.\nStatistics for opcodes in the original binary and the binary with obfuscated source code, including the number of movs, leas, jumps, calls, pushes, and other opcodes that were added.\nThe percentage of changes in the control flow of the code in the IDA code graph for the original binary and the binary with obfuscated source code.\nThe use of automatic tools to simplify, optimize, and deobfuscate the code, and a comparison of the original binary with the optimized one.\n\nBy using these metrics, it is possible to get a more practical and quantitative understanding of the effectiveness of code obfuscation techniques. This can help organizations make informed decisions about how to protect their code and intellectual property.\n",
            "votes": "0",
            "user": "0xC0000022L",
            "time": "Jan 9, 2023 at 8:40",
            "is_accepted": false,
            "comments": [
                {
                    "user": "0xC0000022L",
                    "text": "<span class=\"comment-copy\">Hi and welcome to RE.SE. This sounds a bit vague, except for the reference to the IDA code graph.</span>",
                    "time": null
                },
                {
                    "user": "BenjaminL",
                    "text": "<span class=\"comment-copy\">For visual comparison of blocks similarity IDA is very useful. Also BinDiff can be used for this task.</span>",
                    "time": null
                }
            ]
        }
    ]
}