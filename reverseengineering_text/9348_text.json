{
    "title": "Why are PUSHF and POPF so slow?",
    "link": "https://reverseengineering.stackexchange.com/questions/9348/why-are-pushf-and-popf-so-slow",
    "content": "the experiment is on <pre><code>32-bit</code></pre> <pre><code>x86</code></pre> Linux.\nI am doing some static binary instrumentation work, and basically I am trying to insert some instructions below to the beginning of every basic block. \n<pre><code>BB23 : push %eax\n\nmovl index,%eax\nmovl $0x80823d0,buf(,%eax,0x4)\nadd $0x1,%eax\ncmp $0x400000,%eax\njle BB_23_stub\nmovl $0x0,%eax\nBB_23_stub:movl %eax,index\n\npop %eax\n</code></pre>\nNote that I need to use <pre><code>cmp</code></pre> instruction, and in order to guarantee that <pre><code>flags</code></pre> can restore to the original value, I use <pre><code>pushf</code></pre> and <pre><code>popf</code></pre> to store\\load <pre><code>flags</code></pre> on the stack. \nThen it becomes this:\n<pre><code> BB_23 :    push %eax\n       pushf               \n       movl index,%eax\n       movl $0x17,buf(,%eax,0x4)\n       add $0x1,%eax\n       cmp $0x400000,%eax\n       jle BB_23_stub\n       movl $0x0,%eax\nBB_23_stub:movl %eax,index\n       popf             \n       pop %eax\n</code></pre>\nI tested the performance with and without <pre><code>pushf</code></pre> and <pre><code>popf</code></pre> (I am using <pre><code>gzip</code></pre> and <pre><code>bzip</code></pre>). And to my surprise, performance penalty could increase  even 3 times after using the <pre><code>pushf</code></pre> and <pre><code>popf</code></pre> instructions!!\nHowever, without <pre><code>pushf</code></pre> and <pre><code>popf</code></pre>. The compression results of <pre><code>gzip</code></pre> and <pre><code>bzip</code></pre> are incorrect.\nSo here is my question:\nWhy pushf and popf so slow? Am I using it in a correct way?\nI cannot afford too much performance penalty introduced by pushf and popf. Is there any way I can avoid the high overhead and also keep the correct semantics? (protecting the value in flags, basically..)\nAm I clear enough? Could anyone give me some help?\n",
    "votes": "5",
    "answers": 4,
    "views": "2k",
    "tags": [
        "binary-analysis",
        "x86",
        "instrumentation",
        "binary"
    ],
    "user": "lllllllllllll",
    "time": "Sep 19, 2015 at 2:49",
    "comments": [
        {
            "user": "Guntram Blohm",
            "text": "1) Just an idea without any claim to correctness: <pre><code>pushf</code></pre> might wreck havoc on the instruction pipeline, since it needs all flags valid, while most other instructions don't care about the flags. In the same way, the pipeline might get delayed by popf if an instruction that needs a flag follows. 2) I'd replace your <pre><code>add</code></pre> - <pre><code>cmp</code></pre> - <pre><code>jle</code></pre> - <pre><code>mov</code></pre> combo with <pre><code>inc %eax</code></pre>, <pre><code>and $0x3fffff, %eax</code></pre> which should speed up the code a bit since it avoids a branch. This won't help you with flags, however, I don't see a way to do this without touching flags.\n",
            "time": null
        },
        {
            "user": "Guntram Blohm",
            "text": "Oh, and replacing <pre><code>inc %eax</code></pre>  with <pre><code>lea eax, [eax+1]</code></pre> (sorry, Intel Syntax, I don't really like AT&T syntax and don't know how to translate it right now) will avoid changing the flags like <pre><code>inc</code></pre> does. Now if i could just figure out how to do the <pre><code>and</code></pre> without changing flags and you could get rid of those pesky <pre><code>pushf</code></pre> and <pre><code>popf</code></pre> instructions ...\n",
            "time": null
        },
        {
            "user": "lllllllllllll",
            "text": "@GuntramBlohm. Brilliant!! I really really appreciate your kind help! It really saves my ass..\n",
            "time": null
        },
        {
            "user": "Guntram Blohm",
            "text": "You seem to be starting with index 0, incrementing up to <pre><code>0x400000</code></pre> and wrapping around there. If you can afford to do it the other way round, you could misuse the <pre><code>loop</code></pre> instruction which doesn't change flags. Initialize your index to <pre><code>0x400000</code></pre>, use <pre><code>ecx</code></pre> instead of <pre><code>eax</code></pre>, and to decrement and re-init on zero, use <pre><code>loop forward</code></pre>, <pre><code>mov $0x400000, %ecx</code></pre>, <pre><code>forward: movl %ecx, index</code></pre>. Consider the <pre><code>loop</code></pre> a <pre><code>decrement and jump if not zero</code></pre>.\n",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "Clever (some would say incomprehensible) misuse of x86 features could do this for you. The <pre><code>loop</code></pre> instruction will decrement the <pre><code>ecx</code></pre> register, jump if it's nonzero, and not modify flags. You can use this as a <pre><code>jump forward</code></pre> instruction as well, like this:\n<pre><code>BB23:      push %ecx\n           movl index, %ecx\n           movl $0x17, buf-4(,%ecx,4)\n           loop BB23_stub\n           movl $0x400000, %ecx\nBB23_stub: movl %ecx, index\n           pop %ecx\n</code></pre>\nNote that ecx runs from 0x400000 to 1 here, not from 0 to 0x3fffff, so i had to subtract <pre><code>4</code></pre> from the address of <pre><code>buf</code></pre>, and you need to read the buffer top to bottom when analyzing it. Don't forget to initialize <pre><code>index</code></pre> to <pre><code>0x400000</code></pre> at the start of your code somewhere. You'll have to test how much the penalty of  branching in <pre><code>loop</code></pre> costs in comparison to how much removing <pre><code>pushf</code></pre>/<pre><code>popf</code></pre> gains.\n",
            "votes": "9",
            "user": "Guntram Blohm",
            "time": "Jul 14, 2015 at 19:27",
            "is_accepted": true,
            "comments": [
                {
                    "user": "Peter Cordes",
                    "text": "<span class=\"comment-copy\"><code>loop</code> is really slow (7 uops, throughput of one per 5 cycles), but saving/restoring the flags is even slower.  (<code>pushf</code> = 3 uops, <code>popf</code> = 9).  (Intel SnB/Haswell).</span>",
                    "time": null
                },
                {
                    "user": "Peter Cordes",
                    "text": "<span class=\"comment-copy\">Yes, it sounds like <code>loop</code> is your best option.  Just keep in mind, that one <code>loop</code> is taking more execution resources, and more space in the uop cache, than all 6 other instructions combined.  Hopefully it's still light-weight enough for your purposes.</span>",
                    "time": null
                },
                {
                    "user": "Peter Cordes",
                    "text": "<span class=\"comment-copy\">Possibly faster: <code>push %eax / lahf / use flags / sahf / pop %eax</code>.  load/store AH from/to flags are single-uop, single-cycle-latency instructions on current Intel and AMD CPUs.  If you're instrumenting something that doesn't touch the MMX / x87 registers, you could use them for storing index, and maybe also for masking it after wraparound.  Oh, I see there was already an answer with this.</span>",
                    "time": null
                }
            ]
        },
        {
            "content": "If you look at <pre><code>lib/Target/X86/X86InstrInfo.cpp</code></pre> in the <pre><code>LLVM</code></pre> source code you can see that they prefer the <pre><code>LAHF</code></pre> and <pre><code>SAHF</code></pre> instructions to <pre><code>PUSHF</code></pre> and <pre><code>POPF</code></pre> for speed reasons.  These instructions don't deal with the overflow flag <pre><code>OF</code></pre> so this must be handled with separately.\n<pre><code>alt_pushf:        seto %al                  ; save OF to AL\n                  lahf                      ; save other flags to AH\n                  push %eax                 ; push\n\nalt_popf:         pop %eax                  ; pop\n                  addb $127, %al            ; restore OF\n                  sahf                      ; restore other flags\n</code></pre>\nI don't know if this will be any faster than @GuntramBlohm's clever <pre><code>LOOP</code></pre> option so it might be worth benchmarking.\n(Note that should you want to use this in future for 64-bit code you will need to check for the presence of the <pre><code>LAHF</code></pre> and <pre><code>SAHF</code></pre> instructions.)\n",
            "votes": "4",
            "user": "Ian Cook",
            "time": "Jul 18, 2015 at 20:07",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "Posting a 2nd answer for a different method, combining <pre><code>cmov</code></pre> to avoid a skip-1-instruction branch with @Ian Cook's nice lahf/sahf.\n<pre><code>       push   %ecx\n       movl   index, %ecx\n\n       push   %eax\n       seto   %al            # save OF to AL\n       lahf                  # save other flags to AH\n\n       movl   $0x17,  buf(,%ecx,0x4)\n       dec    %ecx\n       cmovc  buflen, %ecx       # load buflen constant from memory on wraparound\n\n       addb $127, %al            # restore OF\n       sahf                      # restore other flags\n       pop %eax\n\n       movl   %ecx,index\n       pop %ecx\n</code></pre>\nThis is 14 insns, all single-uop single cycle latency (on Intel).  So it's probably still slower than the LOOP version, except for not affecting the branch predictor if this code is duplicated all over the place.\nWith Intel ADX (add-with-carry using CF or OF, to allow two dep chains in parallel), you can avoid clobbering the overflow flag.  But it doesn't take an immediate arg, so you need a constant (-4) in memory.  You need to detect wrapping around zero, and avoid <pre><code>cmp</code></pre>.  This instruction set extension was first supported in Broadwell (barely available for desktops, and not even all currently-for-sale laptops have it.)\nAnyway, <pre><code>clc / adcx  minus_one, %ecx</code></pre>  instead of <pre><code>dec %ecx</code></pre> would save net instructions (one clc to save a <pre><code>seto</code></pre> and <pre><code>addb $127</code></pre> to save/restore the overflow flag), which isn't much.  13 uops is still more than my other answer, using an MMX reg for sub/mask to avoid touching flags.\nAnother possibility is using <pre><code>lea</code></pre>, and zeroing the high bits with a non-flag-affecting left and right shift (BMI2 (Haswell) instruction set's <pre><code>SHLX / SHRX</code></pre>).  This avoids touching flags entirely:\n<pre><code>       push   %ecx\n       movl   index, %ecx\n\n       movl   $0x17,  buf(,%ecx,0x4)\n       lea    -1(%ecx), %ecx\n       push   %eax\n       movl   $bit_count, %eax   # 32 - significant bits in buflen\n       shlx   %eax, %ecx, %ecx   # shift count has to be in a reg\n       shrx   %eax, %ecx, %ecx\n       pop    %eax\n\n       movl   %ecx,index\n       pop %ecx\n</code></pre>\nWell crap, no-flag shifts are only available as (Intel syntax) <pre><code>shrx  r32a,  r/m32,  r32b</code></pre>, loading the the value to be shifted, not the shift count.  And an immediate shift count isn't available either, so I still needed to push/pop eax to get a 2nd register.\nSo this is 11 uops on Intel, all single-cycle latency.  It still doesn't beat the mmx version.\n",
            "votes": "2",
            "user": "Peter Cordes",
            "time": "Jul 22, 2015 at 8:30",
            "is_accepted": false,
            "comments": []
        },
        {
            "content": "What if you have <pre><code>index</code></pre> count downward, and unconditionally mask it to handle wraparound, instead of a conditional?  Hmm, <pre><code>AND</code></pre> sets all the flags, including <pre><code>OF</code></pre> (which isn't saved/restored with <pre><code>lahf/safh</code></pre>).  You could use an MMX register, but <pre><code>PAND</code></pre> doesn't have an immediate form, so you'd need to have the constant in memory.\n<pre><code>BB23:      push %ecx\n           ; movq %mm0, -8(%esp)   ; not safe if a signal handler fires while data is below the stack.\n            ;  x86 has no red-zone.  But we can't sub $16, %esp  without clobbering flags\n           movq   %mm0, save_mm0\n           movd   index, %mm0\n           psubd  one, %mm0      ;  mmx has no dec-by-one\n           pand   my_mask, %mm0   ; (0x400000-1).  0-max -> untouched.  all-1s after wraparound -> max\n           movd   %mm0, %ecx\n           movl   $0x17, buf(,%ecx,4)\n           ; movq   -8(%esp), %mm0\n           movq   save_mm0, %mm0\n           movl   %ecx, index\n           pop    %ecx\n</code></pre>\nOn Intel, this is 10 uops, so it's potentially faster than the version using <pre><code>LOOP</code></pre>.  Or only 8, if the code you're instrumenting doesn't use MMX, or doesn't use SSE, so you could avoid saving / restoring a vector reg.  Jumps interrupt the flow of uops from the decoders or uop cache, so it has that going for it, too.\nIt needs another 8 bytes of constants.  If they're in the same cache-line as index, that's not a big deal.  It does take significantly more instruction bytes.  On the upside, it's branchless, so inserting it all over the place won't pollute the branch predictor with a lot of taken branches.  (Arranging branches so the non-taken case is the common one would be better.  The save/restore flags version could use a cmov from a zeroed memory location, instead of a branch.)\nOn SnB and newer, the scaled-offset version of store might not micro-fuse.   If the immediate data doesn't count as a 3rd input dependency, then it still can.  Otherwise, scale everything up by 4, including the constant for <pre><code>psubd</code></pre>, then the store is <pre><code>movl $0x17, buf(%ecx)</code></pre>.\nMy first version was going to save %mm0 on the stack, but there's no push for MMX regs.  That would have made it 11 uops, counting the stack-engine synchronization uop inserted before the <pre><code>movq %mm0, -8(%rsp)</code></pre>, since it follows a stack instruction (<pre><code>push</code></pre>).\n",
            "votes": "1",
            "user": "Community",
            "time": "May 23, 2017 at 12:37",
            "is_accepted": false,
            "comments": []
        }
    ]
}