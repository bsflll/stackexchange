{
    "title": "reverse engineering file containing time series data",
    "link": "https://reverseengineering.stackexchange.com/questions/13538/reverse-engineering-file-containing-time-series-data",
    "content": "I have to decode a proprietary file format which mainly contains time series data. The only thing I was told was that the content is 'zipped', although I am not sure whether it is really zipped or some in-house compression method is used.\nHere is a snippet of the data:\n<pre><code>789cecdd55d895d5bb057c26489774a7ac798f713feffbd2dd255d4a2b29\n082220a1d24887020648194883747788a4840828024a4a238d347c13beef\n60afb5f777fcac83e7372efedb8d27e3bae635e386d7b592c5cb9f375ebc\n93f1e2c543bc4b09dac5eb302c7f3c5303a910835d32c2f6b579a41bc633\nab66d2945a4c47e95d36e52db44246647789178832c6b03792a0a574b2bb\nb493d194a3f81b9fb3bf1ed734de70adcacdf2a5ac922228122c5f9431ed\n</code></pre>\nI did a [byte] frequency analysis and it shows a uniform distribution. If the bytes are plot (with A.X.E.), a very homogeneous image is shown:\n \nI've checked for the header of common compression algorithms but found none of them.\nAny help is highly appreciated.\nEDIT\nSome of these file can be accessed on the following links: First file, Second file, Third file, Forth file\nThe executable which produced/validated these files is not available.\nEDIT 2\nUsing <pre><code>binwalk -Me <filename></code></pre> (github.com/devttys0/binwalk) it is possible to extract two zlib-ed files from each of the previous files.\nHere's the header of one of the files:\n<pre><code>0a002f220000180100002800e80364001f783c0001a0349c3574367136db\n3582348932bb30dd30a12f2b2f542ff02e432f432ff62edd2f772f6a2f2e\n2fbc2edb2d382dcc2cfe2c272d492d342d1d2d352d352d00000000000000\n000000000000000000000000000000000000000000000000000000000000\n000000000000000000000000000000000000000000000000000000000000\n000000000000000000000000000000000000000000000000000000000000\n000000000000000000000000000000000000000000000000000000000000\n</code></pre>\n",
    "votes": "3",
    "answers": 1,
    "views": "267",
    "tags": [
        "file-format",
        "binary-diagnosis"
    ],
    "user": "izibe",
    "time": "Sep 21, 2016 at 12:40",
    "comments": [
        {
            "user": "NirIzr",
            "text": "Is the code generating or processing the file available to you? how many file samples can you collect? are you able to trigger any validation over modified files? can you share the entire file? more info would help here\n",
            "time": null
        },
        {
            "user": "grepNstepN",
            "text": "sharing the entire file would help immensely. also,what is there once you decompress the zlib'd blobs?\n",
            "time": null
        },
        {
            "user": "Denis Laskov",
            "text": "Best way to analyse proprietary file format - is to analyze software that should receive and process this file format.  I think you should start with RE of binary that able to create file with this file format, read it or convert from\\to it. You have access to those?\n",
            "time": null
        },
        {
            "user": "NirIzr",
            "text": "@DenisLaskov OP already mentioned he does not. \"The executable which produced/validated these files is not available.\" izibe it would be nice of you to upload the uncompressed files, now that you've figured out the first layer.\n",
            "time": null
        }
    ],
    "answers_data": [
        {
            "content": "It would help if I knew what the data represented, but I have determined the format of the files once unzipped.  Each unzipped file has this structure:\n<pre><code>short unk1;           // 10\nshort num_records;    // 8751  (8760 hours per year)\nshort unk2;           // 0\nshort num_samples;    // varies per file from 8 to 179\nshort unk3;           // 0\nshort unk4;           // 40\nshort unk5;           // 1000\nshort unk6;           // 100\nint data_bytecount;   // varies per file (remaining byte count of the file)\nRecord records[num_records];\n</code></pre>\nWhere each Record is defined as: (defined as an 010Editor template)\n<pre><code>typedef struct {\n    byte flag;                // 0 = no data; 1 = sample data follows\n    if(flag == 1)\n        short data[num_samples];\n} Record;\n</code></pre>\nFor example, in the first file unzipped from \"295\", there are 8751 records with 179 samples each.  Most records only have ~30 non-zero values.  The samples for records[0] look like:\n<pre><code>15185, 15185, 15060, 14968, 14719, 14583, ...\n</code></pre>\n\nI suspect the data values are normalized floats, perhaps dividing them by 1000.0 or using 16384 as a zero reference for positive/negative numbers.  I also suspect each file contains 1 year's worth of records with 1 per hour.\nI'd be interested in knowing what the values represent ... just to have a complete picture.\n",
            "votes": "2",
            "user": "David",
            "time": "Sep 24, 2016 at 0:27",
            "is_accepted": false,
            "comments": []
        }
    ]
}